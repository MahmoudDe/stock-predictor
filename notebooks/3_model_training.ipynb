{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Stock Trend Prediction - Model Training\n",
        "\n",
        "- Creates sequences from time series features\n",
        "- Builds and compare neural network architectures (LSTM, GRU)\n",
        "- Trains models using KhayatMiniNN\n",
        "- Compares model performance\n",
        "- Selects best model for prediction\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import pickle\n",
        "import importlib\n",
        "\n",
        "sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath('__file__'))))\n",
        "\n",
        "# Reload KhayatMiniNN modules to pick up any code changes\n",
        "import KhayatMiniNN.optimizers.adam as adam_module\n",
        "importlib.reload(adam_module)\n",
        "\n",
        "from KhayatMiniNN.neural_network import NeuralNetwork\n",
        "from KhayatMiniNN.trainer import Trainer\n",
        "from KhayatMiniNN.layers import LSTM, GRU, Conv1D, Dense, ReLU, Sigmoid, MaxPooling1D\n",
        "from KhayatMiniNN.layers.base import Layer\n",
        "from KhayatMiniNN.regularization import Dropout, BatchNormalization\n",
        "from KhayatMiniNN.losses import BinaryCrossEntropy\n",
        "from KhayatMiniNN.optimizers.adam import Adam  # Import directly from reloaded module\n",
        "from KhayatMiniNN.utils.sequence_utils import create_sequences\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Weighted BinaryCrossEntropy wrapper for class imbalance - IMPROVED VERSION\n",
        "class WeightedBinaryCrossEntropy:\n",
        "    \"\"\"\n",
        "    Improved weighted BinaryCrossEntropy with per-sample weighting and optional focal loss.\n",
        "    Applies different weights to positive and negative classes with per-sample gradient scaling.\n",
        "    \"\"\"\n",
        "    def __init__(self, weight_for_0, weight_for_1, from_logits=False, focal_gamma=0.0):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            weight_for_0: Weight for class 0 (negative/Lower)\n",
        "            weight_for_1: Weight for class 1 (positive/Higher)\n",
        "            from_logits: Whether predictions are logits or probabilities\n",
        "            focal_gamma: Focal loss gamma parameter (0 = no focal loss, 2.0 = strong focus on hard examples)\n",
        "        \"\"\"\n",
        "        self.base_loss = BinaryCrossEntropy(from_logits=from_logits)\n",
        "        self.weight_for_0 = weight_for_0\n",
        "        self.weight_for_1 = weight_for_1\n",
        "        self.from_logits = from_logits\n",
        "        self.focal_gamma = focal_gamma\n",
        "        self.predictions = None\n",
        "        self.targets = None\n",
        "        self.weights = None\n",
        "    \n",
        "    def forward(self, predictions, targets):\n",
        "        \"\"\"Compute weighted binary cross-entropy loss with optional focal loss.\"\"\"\n",
        "        self.predictions = predictions\n",
        "        self.targets = targets\n",
        "        \n",
        "        # Ensure correct shapes\n",
        "        if targets.ndim == 1:\n",
        "            targets_reshaped = targets.reshape(-1, 1)\n",
        "        else:\n",
        "            targets_reshaped = targets\n",
        "        \n",
        "        if predictions.ndim == 1:\n",
        "            predictions_reshaped = predictions.reshape(-1, 1)\n",
        "        else:\n",
        "            predictions_reshaped = predictions\n",
        "        \n",
        "        # Clip predictions to avoid log(0)\n",
        "        eps = 1e-7\n",
        "        predictions_clipped = np.clip(predictions_reshaped, eps, 1 - eps)\n",
        "        \n",
        "        # Calculate per-sample BCE loss\n",
        "        bce = -(targets_reshaped * np.log(predictions_clipped) + \n",
        "                (1 - targets_reshaped) * np.log(1 - predictions_clipped))\n",
        "        \n",
        "        # Calculate per-sample weights\n",
        "        self.weights = np.where(targets_reshaped == 0, self.weight_for_0, self.weight_for_1)\n",
        "        \n",
        "        # Apply focal loss modulation if gamma > 0\n",
        "        if self.focal_gamma > 0:\n",
        "            # pt = p if y=1, else (1-p)\n",
        "            pt = np.where(targets_reshaped == 1, predictions_clipped, 1 - predictions_clipped)\n",
        "            focal_weight = (1 - pt) ** self.focal_gamma\n",
        "            bce = focal_weight * bce\n",
        "        \n",
        "        # Apply class weights per sample\n",
        "        weighted_bce = self.weights * bce\n",
        "        \n",
        "        return np.mean(weighted_bce)\n",
        "    \n",
        "    def backward(self):\n",
        "        \"\"\"Return weighted gradient.\"\"\"\n",
        "        eps = 1e-7\n",
        "        predictions = self.predictions\n",
        "        targets = self.targets\n",
        "        \n",
        "        if targets.ndim == 1:\n",
        "            targets = targets.reshape(-1, 1)\n",
        "        if predictions.ndim == 1:\n",
        "            predictions = predictions.reshape(-1, 1)\n",
        "        \n",
        "        predictions_clipped = np.clip(predictions, eps, 1 - eps)\n",
        "        \n",
        "        # Base gradient: (p - y) / (p * (1 - p))\n",
        "        grad = (predictions_clipped - targets) / (predictions_clipped * (1 - predictions_clipped) + eps)\n",
        "        \n",
        "        # Apply class weights to gradients\n",
        "        weighted_grad = self.weights * grad / targets.shape[0]\n",
        "        \n",
        "        return weighted_grad\n",
        "    \n",
        "    def set_loss(self, loss_fn):\n",
        "        \"\"\"Compatibility method - allows setting base loss if needed.\"\"\"\n",
        "        pass\n",
        "\n",
        "class Flatten(Layer):\n",
        "    \"\"\"Flatten layer to convert 3D (batch, seq, features) to 2D (batch, seq*features).\"\"\"\n",
        "    def __init__(self, name=\"Flatten\"):\n",
        "        super().__init__(name)\n",
        "    \n",
        "    def forward(self, input_data):\n",
        "        self.input = input_data\n",
        "        batch_size = input_data.shape[0]\n",
        "        return input_data.reshape(batch_size, -1)\n",
        "    \n",
        "    def backward(self, output_grad):\n",
        "        return output_grad.reshape(self.input.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load Feature-Engineered Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train: 5,505,350 samples. Val: 5,475,700. Features: 71.\n"
          ]
        }
      ],
      "source": [
        "data_dir = Path(\"../data/processed\")\n",
        "train_df = pd.read_csv(data_dir / \"train_features.csv\")\n",
        "val_df = pd.read_csv(data_dir / \"val_features.csv\")\n",
        "\n",
        "with open(data_dir / \"feature_cols.pkl\", \"rb\") as f:\n",
        "    feature_cols = pickle.load(f)\n",
        "\n",
        "# Ensure the CSV has all columns listed in feature_cols (avoids KeyError later)\n",
        "missing = [c for c in feature_cols if c not in train_df.columns]\n",
        "if missing:\n",
        "    raise KeyError(\n",
        "        \"Some features in feature_cols.pkl are missing from train_features.csv, e.g.: \"\n",
        "        + str(missing[:10])\n",
        "        + \". Re-run the feature engineering notebook (2_feature_engineering) to regenerate the processed files.\"\n",
        "    )\n",
        "\n",
        "print(f\"Train: {len(train_df):,} samples. Val: {len(val_df):,}. Features: {len(feature_cols)}.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Create Sequences\n",
        "\n",
        "We'll create sequences from the time series data. Each sequence will have a fixed length (e.g., 30 days) and predict the target.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "Creating Sequences\n",
            "============================================================\n",
            "\n",
            "Using MAX_SAMPLES = 150,000 (â‰ˆ1.9 GB)\n",
            "\n",
            "Creating sequences for train set...\n",
            "  Creating sequences per ticker...\n",
            "  Completed: 150,000 sequences created\n",
            "Train sequences: (150000, 20, 71)\n",
            "Memory usage: 0.79 GB (float32)\n",
            "\n",
            "Creating sequences for validation set...\n",
            "  Creating sequences per ticker...\n",
            "  Completed: 15,000 sequences created\n",
            "Val sequences: (15000, 20, 71)\n",
            "Memory usage: 0.08 GB (float32)\n",
            "\n",
            "Class weights (for imbalanced data):\n",
            "  Class 0 (Lower): 1.0985\n",
            "  Class 1 (Higher): 0.9177\n",
            "\n",
            "Target distribution (train):\n",
            "(Higher): 81722 (54.48%)\n",
            "(Lower): 68278 (45.52%)\n",
            "\n",
            "Target distribution (val):\n",
            "(Higher): 8657 (57.71%)\n",
            "(Lower): 6343 (42.29%)\n",
            "Dataset limited to 150,000 samples per set.\n",
            "\n",
            "âœ“ Freed memory from dataframes\n"
          ]
        }
      ],
      "source": [
        "# Training Parameters - TUNED FOR BETTER RESULTS\n",
        "SEQUENCE_LENGTH = 20  # Reduced from 30 to 20 - shorter sequences reduce noise and overfitting\n",
        "BATCH_SIZE = 64  # Increased from 16 to 64 - larger batches provide more stable gradients\n",
        "EPOCHS = 30  # Increased from 20 to 30 - allow more training time with lower LR\n",
        "LEARNING_RATE = 0.0002  # Reduced from 0.0005 for smoother convergence and less overfitting\n",
        "EARLY_STOPPING_PATIENCE = 5  # Reduced to 5 to stop before overfitting kicks in\n",
        "\n",
        "# Memory management: Limit dataset size to avoid memory errors and disk space issues\n",
        "# 100000 samples â‰ˆ 1.3 GB, 150000 samples â‰ˆ 1.9 GB, 200000 samples â‰ˆ 2.6 GB\n",
        "# Further reduced to prevent macOS swap file creation (which consumes disk space)\n",
        "MAX_SAMPLES = 150000  # Increased to 150k for better generalization (more data helps reduce overfitting)\n",
        "SAMPLE_RATE = 1.0  # Sample rate for sequences (1.0 = all, 0.1 = 10% of sequences)\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# In-memory sequence creation (no disk storage)\n",
        "def create_sequences_in_memory(df, feature_cols, sequence_length, target_col='target', \n",
        "                               max_samples=None, sample_rate=1.0):\n",
        "    \"\"\"\n",
        "    Create sequences in memory grouped by ticker.\n",
        "    Optionally limit dataset size with max_samples or sample_rate.\n",
        "    Uses pre-allocated arrays when max_samples is set for better memory efficiency.\n",
        "    \"\"\"\n",
        "    if max_samples is None:\n",
        "        raise ValueError(\"max_samples must be set to avoid memory errors. Try MAX_SAMPLES = 500000 or adjust based on available RAM.\")\n",
        "    \n",
        "    # Pre-allocate arrays for better memory management\n",
        "    X = np.zeros((max_samples, sequence_length, len(feature_cols)), dtype=np.float32)\n",
        "    y = np.zeros(max_samples, dtype=np.float32)\n",
        "    current_idx = 0\n",
        "    \n",
        "    print(\"  Creating sequences per ticker...\")\n",
        "    tickers = df['Ticker'].unique()\n",
        "    \n",
        "    for ticker_idx, ticker in enumerate(tickers):\n",
        "        if current_idx >= max_samples:\n",
        "            break\n",
        "            \n",
        "        ticker_data = df[df['Ticker'] == ticker].sort_values('Date')\n",
        "        features = ticker_data[feature_cols].values.astype(np.float32)\n",
        "        targets = ticker_data[target_col].values.astype(np.float32)\n",
        "        \n",
        "        if len(features) < sequence_length:\n",
        "            continue\n",
        "        \n",
        "        # Create sliding window sequences for this ticker\n",
        "        num_sequences = len(features) - sequence_length + 1\n",
        "        \n",
        "        # Apply sampling if needed\n",
        "        if sample_rate < 1.0:\n",
        "            num_sequences = int(num_sequences * sample_rate)\n",
        "            indices = np.random.choice(len(features) - sequence_length + 1, \n",
        "                                      size=num_sequences, replace=False)\n",
        "            indices = np.sort(indices)\n",
        "        else:\n",
        "            indices = np.arange(len(features) - sequence_length + 1)\n",
        "        \n",
        "        for i in indices:\n",
        "            if current_idx >= max_samples:\n",
        "                break\n",
        "            X[current_idx] = features[i:i+sequence_length]\n",
        "            y[current_idx] = targets[i+sequence_length-1]\n",
        "            current_idx += 1\n",
        "        \n",
        "        if (ticker_idx + 1) % 100 == 0:\n",
        "            print(f\"  Processed {ticker_idx + 1}/{len(tickers)} tickers, {current_idx:,} sequences...\", end='\\r')\n",
        "    \n",
        "    print(f\"  Completed: {current_idx:,} sequences created\")\n",
        "    \n",
        "    # Trim to actual size\n",
        "    if current_idx < max_samples:\n",
        "        X = X[:current_idx]\n",
        "        y = y[:current_idx]\n",
        "    \n",
        "    return X, y\n",
        "\n",
        "seq_dir = Path(\"../data/sequences\")\n",
        "if seq_dir.exists():\n",
        "    print(\"Cleaning up existing sequence files to free disk space...\")\n",
        "    for file in seq_dir.glob(\"*.dat\"):\n",
        "        try:\n",
        "            file.unlink()\n",
        "            print(f\"  Deleted: {file.name}\")\n",
        "        except Exception as e:\n",
        "            print(f\"  Could not delete {file.name}: {e}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"Creating Sequences\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"\\nUsing MAX_SAMPLES = {MAX_SAMPLES:,} (â‰ˆ{MAX_SAMPLES * 30 * 115 * 4 / 1024**3:.1f} GB)\")\n",
        "\n",
        "print(\"\\nCreating sequences for train set...\")\n",
        "X_train, y_train = create_sequences_in_memory(\n",
        "    train_df, feature_cols, SEQUENCE_LENGTH, \n",
        "    max_samples=MAX_SAMPLES, sample_rate=SAMPLE_RATE\n",
        ")\n",
        "print(f\"Train sequences: {X_train.shape}\")\n",
        "print(f\"Memory usage: {X_train.nbytes / 1024**3:.2f} GB (float32)\")\n",
        "\n",
        "print(\"\\nCreating sequences for validation set...\")\n",
        "# Use same sampling for validation, but limit to reasonable size\n",
        "val_max_samples = MAX_SAMPLES // 10 if MAX_SAMPLES else None  # 10% of train size\n",
        "X_val, y_val = create_sequences_in_memory(\n",
        "    val_df, feature_cols, SEQUENCE_LENGTH,\n",
        "    max_samples=val_max_samples, sample_rate=SAMPLE_RATE\n",
        ")\n",
        "print(f\"Val sequences: {X_val.shape}\")\n",
        "print(f\"Memory usage: {X_val.nbytes / 1024**3:.2f} GB (float32)\")\n",
        "\n",
        "# Calculate class weights for imbalanced data\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "# Calculate class weights\n",
        "classes = np.unique(y_train)\n",
        "class_weights = compute_class_weight('balanced', classes=classes, y=y_train)\n",
        "class_weight_dict = dict(zip(classes, class_weights))\n",
        "\n",
        "print(f\"\\nClass weights (for imbalanced data):\")\n",
        "print(f\"  Class 0 (Lower): {class_weight_dict[0]:.4f}\")\n",
        "print(f\"  Class 1 (Higher): {class_weight_dict[1]:.4f}\")\n",
        "\n",
        "# Store for use in training\n",
        "weight_for_0 = class_weight_dict[0]\n",
        "weight_for_1 = class_weight_dict[1]\n",
        "\n",
        "print(f\"\\nTarget distribution (train):\")\n",
        "print(f\"(Higher): {y_train.sum():.0f} ({y_train.mean()*100:.2f}%)\")\n",
        "print(f\"(Lower): {(len(y_train) - y_train.sum()):.0f} ({(1-y_train.mean())*100:.2f}%)\")\n",
        "\n",
        "print(f\"\\nTarget distribution (val):\")\n",
        "print(f\"(Higher): {y_val.sum():.0f} ({y_val.mean()*100:.2f}%)\")\n",
        "print(f\"(Lower): {(len(y_val) - y_val.sum()):.0f} ({(1-y_val.mean())*100:.2f}%)\")\n",
        "\n",
        "if MAX_SAMPLES:\n",
        "    print(f\"Dataset limited to {MAX_SAMPLES:,} samples per set.\")\n",
        "if SAMPLE_RATE < 1.0:\n",
        "    print(f\"Using {SAMPLE_RATE*100:.1f}% of available sequences.\")\n",
        "\n",
        "# Free up memory by deleting dataframes (sequences are already created)\n",
        "del train_df, val_df\n",
        "import gc\n",
        "gc.collect()\n",
        "print(\"\\nâœ“ Freed memory from dataframes\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Build Multiple Model Architectures\n",
        "\n",
        "We'll create and train multiple models:\n",
        "1. **LSTM** - Long Short-Term Memory\n",
        "2. **GRU** - Gated Recurrent Unit  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Model architectures defined. Ready to train!\n"
          ]
        }
      ],
      "source": [
        "# Model parameters - TUNED FOR BETTER RESULTS\n",
        "input_size = len(feature_cols)\n",
        "hidden_size = 64\n",
        "# Further reduced hidden sizes to prevent overfitting\n",
        "lstm_hidden = 32  # Reduced from 48 to 32 for better generalization\n",
        "gru_hidden = 48  # Reduced from 64 to 48 to match LSTM complexity\n",
        "conv_filters = 32\n",
        "\n",
        "# Focal loss gamma - helps focus on hard-to-classify examples (reduces bias toward majority class)\n",
        "FOCAL_GAMMA = 1.0  # Reduced from 1.5 to 1.0 to prevent GRU collapse\n",
        "\n",
        "# Class weight multiplier - boost weight for minority class to balance predictions\n",
        "CLASS_WEIGHT_BOOST = 1.2  # Reduced from 1.5 to 1.2 for more stable training\n",
        "\n",
        "# L2 regularization (weight decay) - helps prevent overfitting\n",
        "L2_WEIGHT_DECAY = 0.001  # Small L2 penalty on weights\n",
        "\n",
        "# Helper function to create and train a model\n",
        "def build_and_train_model(model_name, architecture_func, X_train, y_train, X_val, y_val, \n",
        "                         use_weighted_loss=True, use_focal_loss=True, custom_lr=None):\n",
        "    \"\"\"Build, train, and evaluate a model with optional weighted loss for class imbalance.\n",
        "    \n",
        "    Args:\n",
        "        use_focal_loss: If False, uses standard weighted BCE without focal modulation (helps GRU)\n",
        "        custom_lr: Optional custom learning rate for this model\n",
        "    \"\"\"\n",
        "    import gc\n",
        "    import sys\n",
        "    \n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Training {model_name}\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    # Build model\n",
        "    model = architecture_func()\n",
        "    model.summary()\n",
        "    \n",
        "    # Set loss - use weighted loss with optional focal loss\n",
        "    if use_weighted_loss and 'weight_for_0' in globals() and 'weight_for_1' in globals():\n",
        "        # Boost the minority class weight to balance predictions\n",
        "        boosted_weight_0 = weight_for_0 * CLASS_WEIGHT_BOOST\n",
        "        focal_gamma = FOCAL_GAMMA if use_focal_loss else 0.0\n",
        "        loss_fn = WeightedBinaryCrossEntropy(\n",
        "            weight_for_0=boosted_weight_0, \n",
        "            weight_for_1=weight_for_1, \n",
        "            from_logits=False,\n",
        "            focal_gamma=focal_gamma\n",
        "        )\n",
        "        if use_focal_loss:\n",
        "            print(f\"\\nâœ“ Using weighted focal loss (gamma={focal_gamma})\")\n",
        "        else:\n",
        "            print(f\"\\nâœ“ Using weighted BCE (no focal loss - more stable for some architectures)\")\n",
        "        print(f\"  Class 0 weight: {boosted_weight_0:.4f} (boosted), Class 1 weight: {weight_for_1:.4f}\")\n",
        "    else:\n",
        "        loss_fn = BinaryCrossEntropy(from_logits=False)\n",
        "        print(f\"\\nâš  Using standard loss (no class weights)\")\n",
        "    \n",
        "    model.set_loss(loss_fn)\n",
        "    \n",
        "    # Create optimizer and trainer with L2 regularization (weight decay)\n",
        "    lr = custom_lr if custom_lr is not None else LEARNING_RATE\n",
        "    optimizer = Adam(learning_rate=lr, weight_decay=L2_WEIGHT_DECAY)\n",
        "    trainer = Trainer(model, optimizer, loss_fn)\n",
        "    print(f\"  Learning rate: {lr}\")\n",
        "    \n",
        "    # Train with early stopping - now tracking best validation accuracy (better for classification)\n",
        "    best_val_acc = 0.0\n",
        "    best_val_loss = float('inf')\n",
        "    patience_counter = 0\n",
        "    best_params = None\n",
        "    \n",
        "    print(f\"\\nTraining for up to {EPOCHS} epochs...\")\n",
        "    print(f\"Early stopping based on validation accuracy (patience={EARLY_STOPPING_PATIENCE})\")\n",
        "    \n",
        "    for epoch in range(EPOCHS):\n",
        "        # Aggressive memory cleanup before each epoch\n",
        "        gc.collect()\n",
        "        \n",
        "        # Train one epoch\n",
        "        history = trainer.fit(\n",
        "            X_train, y_train.reshape(-1, 1),\n",
        "            X_val=X_val, y_val=y_val.reshape(-1, 1),\n",
        "            epochs=1,\n",
        "            batch_size=BATCH_SIZE,\n",
        "            verbose=False\n",
        "        )\n",
        "        \n",
        "        # Get validation metrics\n",
        "        val_loss, val_acc = trainer.evaluate(X_val, y_val.reshape(-1, 1))\n",
        "        \n",
        "        # Calculate class-specific metrics every few epochs\n",
        "        if (epoch + 1) % 3 == 0 or epoch == 0:\n",
        "            train_loss, train_acc = trainer.evaluate(X_train, y_train.reshape(-1, 1))\n",
        "            \n",
        "            # Get predictions for class-specific metrics\n",
        "            y_val_pred = (trainer.predict(X_val) > 0.5).astype(int).flatten()\n",
        "            class_0_acc = np.mean(y_val_pred[y_val == 0] == 0) * 100 if np.sum(y_val == 0) > 0 else 0\n",
        "            class_1_acc = np.mean(y_val_pred[y_val == 1] == 1) * 100 if np.sum(y_val == 1) > 0 else 0\n",
        "            \n",
        "            print(f\"Epoch {epoch+1}/{EPOCHS} | \"\n",
        "                  f\"Train: {train_acc:.1f}% | Val: {val_acc:.1f}% | \"\n",
        "                  f\"Gap: {train_acc - val_acc:+.1f}% | \"\n",
        "                  f\"Class0: {class_0_acc:.1f}% | Class1: {class_1_acc:.1f}%\")\n",
        "        \n",
        "        # Early stopping based on validation accuracy (better than loss for classification)\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            best_val_loss = val_loss\n",
        "            patience_counter = 0\n",
        "            # Store best params\n",
        "            best_params = model.get_params()\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= EARLY_STOPPING_PATIENCE:\n",
        "                print(f\"\\nEarly stopping at epoch {epoch+1} (no improvement for {EARLY_STOPPING_PATIENCE} epochs)\")\n",
        "                break\n",
        "        \n",
        "        # Force garbage collection every epoch\n",
        "        gc.collect()\n",
        "    \n",
        "    # Restore best parameters\n",
        "    if best_params:\n",
        "        model.set_params(best_params)\n",
        "    \n",
        "    # Final evaluation\n",
        "    val_loss, val_accuracy = trainer.evaluate(X_val, y_val.reshape(-1, 1))\n",
        "    train_loss, train_accuracy = trainer.evaluate(X_train, y_train.reshape(-1, 1))\n",
        "    \n",
        "    # Final class-specific metrics\n",
        "    y_val_pred = (trainer.predict(X_val) > 0.5).astype(int).flatten()\n",
        "    final_class_0_acc = np.mean(y_val_pred[y_val == 0] == 0) * 100 if np.sum(y_val == 0) > 0 else 0\n",
        "    final_class_1_acc = np.mean(y_val_pred[y_val == 1] == 1) * 100 if np.sum(y_val == 1) > 0 else 0\n",
        "    \n",
        "    print(f\"\\nâœ“ Training completed!\")\n",
        "    print(f\"  Best Val Accuracy: {best_val_acc:.2f}% (Loss: {best_val_loss:.4f})\")\n",
        "    print(f\"  Final Val Accuracy: {val_accuracy:.2f}%\")\n",
        "    print(f\"  Train Accuracy: {train_accuracy:.2f}%\")\n",
        "    print(f\"  Class 0 (Lower) Accuracy: {final_class_0_acc:.2f}%\")\n",
        "    print(f\"  Class 1 (Higher) Accuracy: {final_class_1_acc:.2f}%\")\n",
        "    \n",
        "    # Clear trainer optimizer states to free memory\n",
        "    if hasattr(trainer, 'layer_optimizers'):\n",
        "        trainer.layer_optimizers.clear()\n",
        "    \n",
        "    return model, trainer, {\n",
        "        'name': model_name,\n",
        "        'val_loss': best_val_loss,\n",
        "        'val_accuracy': best_val_acc,  # Use best accuracy instead of final\n",
        "        'train_accuracy': train_accuracy,\n",
        "        'class_0_accuracy': final_class_0_acc,\n",
        "        'class_1_accuracy': final_class_1_acc,\n",
        "        'history': history\n",
        "    }\n",
        "\n",
        "# Define model architectures - TUNED FOR BETTER RESULTS\n",
        "def build_lstm_model():\n",
        "    \"\"\"Build optimized LSTM model with strong regularization to prevent overfitting.\"\"\"\n",
        "    model = NeuralNetwork(name=\"LSTM_Stock_Predictor_V2\")\n",
        "    # LSTM layer with smaller hidden size\n",
        "    model.add_layer(LSTM(input_size, lstm_hidden, return_sequences=False), name=\"lstm1\")\n",
        "    # Batch normalization for stable training\n",
        "    model.add_layer(BatchNormalization(lstm_hidden), name=\"batchnorm1\")\n",
        "    # Higher dropout to aggressively prevent overfitting\n",
        "    model.add_layer(Dropout(dropout_rate=0.6), name=\"dropout1\")\n",
        "    # First dense layer - smaller size for simpler model\n",
        "    model.add_layer(Dense(lstm_hidden, 16), name=\"dense1\")\n",
        "    model.add_layer(BatchNormalization(16), name=\"batchnorm2\")\n",
        "    model.add_layer(ReLU(), name=\"relu1\")\n",
        "    # Strong dropout before output\n",
        "    model.add_layer(Dropout(dropout_rate=0.5), name=\"dropout2\")\n",
        "    # Output layer\n",
        "    model.add_layer(Dense(16, 1), name=\"dense2\")\n",
        "    model.add_layer(Sigmoid(), name=\"sigmoid1\")\n",
        "    return model\n",
        "\n",
        "def build_gru_model():\n",
        "    \"\"\"Build GRU model - simplified architecture to prevent collapse.\"\"\"\n",
        "    model = NeuralNetwork(name=\"GRU_Stock_Predictor_V3\")\n",
        "    # GRU layer - simpler, larger hidden size for better gradient flow\n",
        "    model.add_layer(GRU(input_size, 64, return_sequences=False), name=\"gru1\")\n",
        "    # Lower dropout to prevent gradient starvation\n",
        "    model.add_layer(Dropout(dropout_rate=0.4), name=\"dropout1\")\n",
        "    # Direct to output with one hidden layer - simpler architecture\n",
        "    model.add_layer(Dense(64, 32), name=\"dense1\")\n",
        "    model.add_layer(ReLU(), name=\"relu1\")\n",
        "    model.add_layer(Dropout(dropout_rate=0.3), name=\"dropout2\")\n",
        "    model.add_layer(Dense(32, 1), name=\"dense2\")\n",
        "    model.add_layer(Sigmoid(), name=\"sigmoid1\")\n",
        "    return model\n",
        "\n",
        "def build_conv1d_model():\n",
        "    \"\"\"Build Conv1D model with proper shape handling - TUNED.\"\"\"\n",
        "    # Calculate output shape after convolutions and pooling\n",
        "    # Input: (batch, SEQUENCE_LENGTH, input_size)\n",
        "    # After conv1 (same padding): (batch, seq_len, conv_filters)\n",
        "    # After pool1 (pool_size=2, stride=2): (batch, seq_len//2, conv_filters)\n",
        "    # After conv2 (same padding): (batch, seq_len//2, conv_filters*2)\n",
        "    # After pool2 (pool_size=2, stride=2): (batch, seq_len//4, conv_filters*2)\n",
        "    \n",
        "    seq_after_pool1 = (SEQUENCE_LENGTH - 2) // 2 + 1\n",
        "    seq_after_pool2 = (seq_after_pool1 - 2) // 2 + 1\n",
        "    flattened_size = seq_after_pool2 * (conv_filters * 2)\n",
        "    \n",
        "    model = NeuralNetwork(name=\"Conv1D_Stock_Predictor_V2\")\n",
        "    model.add_layer(Conv1D(input_size, conv_filters, kernel_size=3, padding='same'), name=\"conv1\")\n",
        "    model.add_layer(BatchNormalization(conv_filters), name=\"batchnorm1\")\n",
        "    model.add_layer(ReLU(), name=\"relu1\")\n",
        "    model.add_layer(MaxPooling1D(pool_size=2, stride=2), name=\"pool1\")\n",
        "    model.add_layer(Dropout(dropout_rate=0.3), name=\"dropout1\")\n",
        "    model.add_layer(Conv1D(conv_filters, conv_filters*2, kernel_size=3, padding='same'), name=\"conv2\")\n",
        "    model.add_layer(BatchNormalization(conv_filters*2), name=\"batchnorm2\")\n",
        "    model.add_layer(ReLU(), name=\"relu2\")\n",
        "    model.add_layer(MaxPooling1D(pool_size=2, stride=2), name=\"pool2\")\n",
        "    model.add_layer(Flatten(), name=\"flatten\")\n",
        "    model.add_layer(Dense(flattened_size, 32), name=\"dense1\")\n",
        "    model.add_layer(BatchNormalization(32), name=\"batchnorm3\")\n",
        "    model.add_layer(ReLU(), name=\"relu3\")\n",
        "    model.add_layer(Dropout(dropout_rate=0.5), name=\"dropout2\")\n",
        "    model.add_layer(Dense(32, 1), name=\"dense2\")\n",
        "    model.add_layer(Sigmoid(), name=\"sigmoid1\")\n",
        "    return model\n",
        "\n",
        "def build_hybrid_model():\n",
        "    \"\"\"Build Conv1D + LSTM hybrid model - TUNED.\"\"\"\n",
        "    model = NeuralNetwork(name=\"Hybrid_ConvLSTM_V2\")\n",
        "    # Conv1D layers for feature extraction\n",
        "    model.add_layer(Conv1D(input_size, conv_filters, kernel_size=3, padding='same'), name=\"conv1\")\n",
        "    model.add_layer(BatchNormalization(conv_filters), name=\"batchnorm1\")\n",
        "    model.add_layer(ReLU(), name=\"relu1\")\n",
        "    model.add_layer(MaxPooling1D(pool_size=2, stride=2), name=\"pool1\")\n",
        "    model.add_layer(Dropout(dropout_rate=0.3), name=\"dropout1\")\n",
        "    # LSTM for sequence modeling\n",
        "    model.add_layer(LSTM(conv_filters, lstm_hidden, return_sequences=False), name=\"lstm1\")\n",
        "    model.add_layer(BatchNormalization(lstm_hidden), name=\"batchnorm2\")\n",
        "    model.add_layer(Dropout(dropout_rate=0.6), name=\"dropout2\")\n",
        "    model.add_layer(Dense(lstm_hidden, 16), name=\"dense1\")\n",
        "    model.add_layer(ReLU(), name=\"relu2\")\n",
        "    model.add_layer(Dropout(dropout_rate=0.5), name=\"dropout3\")\n",
        "    model.add_layer(Dense(16, 1), name=\"dense2\")\n",
        "    model.add_layer(Sigmoid(), name=\"sigmoid1\")\n",
        "    return model\n",
        "\n",
        "print(\"\\nModel architectures defined. Ready to train!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Train All Models\n",
        "\n",
        "We'll train all model architectures and compare their performance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "Training LSTM...\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "Training LSTM\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "Network: LSTM_Stock_Predictor_V2\n",
            "============================================================\n",
            "Layer Name           Type                 Parameters          \n",
            "------------------------------------------------------------\n",
            "lstm1                LSTM                 13312               \n",
            "batchnorm1           BatchNormalization   64                  \n",
            "dropout1             Dropout              0                   \n",
            "dense1               Dense                528                 \n",
            "batchnorm2           BatchNormalization   32                  \n",
            "relu1                ReLU                 0                   \n",
            "dropout2             Dropout              0                   \n",
            "dense2               Dense                17                  \n",
            "sigmoid1             Sigmoid              0                   \n",
            "------------------------------------------------------------\n",
            "Total parameters: 13953\n",
            "============================================================\n",
            "\n",
            "\n",
            "âœ“ Using weighted focal loss (gamma=1.0)\n",
            "  Class 0 weight: 1.3181 (boosted), Class 1 weight: 0.9177\n",
            "  Learning rate: 0.0002\n",
            "\n",
            "Training for up to 30 epochs...\n",
            "Early stopping based on validation accuracy (patience=5)\n",
            "Epoch 1/30 | Train: 50.0% | Val: 42.4% | Gap: +7.5% | Class0: 97.4% | Class1: 2.1%\n",
            "Epoch 3/30 | Train: 51.2% | Val: 43.3% | Gap: +7.9% | Class0: 96.9% | Class1: 4.0%\n",
            "Epoch 6/30 | Train: 57.5% | Val: 44.6% | Gap: +12.8% | Class0: 83.0% | Class1: 16.5%\n",
            "Epoch 9/30 | Train: 62.2% | Val: 49.8% | Gap: +12.4% | Class0: 54.5% | Class1: 46.3%\n",
            "Epoch 12/30 | Train: 65.0% | Val: 51.4% | Gap: +13.7% | Class0: 51.9% | Class1: 51.0%\n",
            "Epoch 15/30 | Train: 67.7% | Val: 52.6% | Gap: +15.1% | Class0: 43.5% | Class1: 59.2%\n",
            "Epoch 18/30 | Train: 68.8% | Val: 54.0% | Gap: +14.8% | Class0: 41.2% | Class1: 63.3%\n",
            "Epoch 21/30 | Train: 70.4% | Val: 53.6% | Gap: +16.7% | Class0: 37.5% | Class1: 65.5%\n",
            "\n",
            "Early stopping at epoch 23 (no improvement for 5 epochs)\n",
            "\n",
            "âœ“ Training completed!\n",
            "  Best Val Accuracy: 53.98% (Loss: 0.4410)\n",
            "  Final Val Accuracy: 55.15%\n",
            "  Train Accuracy: 69.88%\n",
            "  Class 0 (Lower) Accuracy: 32.93%\n",
            "  Class 1 (Higher) Accuracy: 71.42%\n",
            "âœ“ Memory freed after LSTM training\n",
            "\n",
            "============================================================\n",
            "Training GRU (no focal loss for stability)...\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "Training GRU\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "Network: GRU_Stock_Predictor_V3\n",
            "============================================================\n",
            "Layer Name           Type                 Parameters          \n",
            "------------------------------------------------------------\n",
            "gru1                 GRU                  26112               \n",
            "dropout1             Dropout              0                   \n",
            "dense1               Dense                2080                \n",
            "relu1                ReLU                 0                   \n",
            "dropout2             Dropout              0                   \n",
            "dense2               Dense                33                  \n",
            "sigmoid1             Sigmoid              0                   \n",
            "------------------------------------------------------------\n",
            "Total parameters: 28225\n",
            "============================================================\n",
            "\n",
            "\n",
            "âœ“ Using weighted BCE (no focal loss - more stable for some architectures)\n",
            "  Class 0 weight: 1.3181 (boosted), Class 1 weight: 0.9177\n",
            "  Learning rate: 0.0005\n",
            "\n",
            "Training for up to 30 epochs...\n",
            "Early stopping based on validation accuracy (patience=5)\n",
            "Epoch 1/30 | Train: 50.1% | Val: 42.6% | Gap: +7.5% | Class0: 96.6% | Class1: 3.0%\n",
            "Epoch 3/30 | Train: 50.9% | Val: 43.0% | Gap: +7.9% | Class0: 93.5% | Class1: 6.1%\n",
            "Epoch 6/30 | Train: 51.6% | Val: 42.8% | Gap: +8.8% | Class0: 93.4% | Class1: 5.7%\n",
            "Epoch 9/30 | Train: 51.4% | Val: 43.3% | Gap: +8.0% | Class0: 97.3% | Class1: 3.8%\n",
            "Epoch 12/30 | Train: 51.2% | Val: 42.9% | Gap: +8.3% | Class0: 93.2% | Class1: 6.1%\n",
            "\n",
            "Early stopping at epoch 14 (no improvement for 5 epochs)\n",
            "\n",
            "âœ“ Training completed!\n",
            "  Best Val Accuracy: 43.33% (Loss: 0.7707)\n",
            "  Final Val Accuracy: 43.33%\n",
            "  Train Accuracy: 51.37%\n",
            "  Class 0 (Lower) Accuracy: 97.32%\n",
            "  Class 1 (Higher) Accuracy: 3.77%\n",
            "âœ“ Memory freed after GRU training\n",
            "\n",
            "============================================================\n",
            "Training Conv1D...\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "Training Conv1D\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "Network: Conv1D_Stock_Predictor_V2\n",
            "============================================================\n",
            "Layer Name           Type                 Parameters          \n",
            "------------------------------------------------------------\n",
            "conv1                Conv1D               6848                \n",
            "batchnorm1           BatchNormalization   64                  \n",
            "relu1                ReLU                 0                   \n",
            "pool1                MaxPooling1D         0                   \n",
            "dropout1             Dropout              0                   \n",
            "conv2                Conv1D               6208                \n",
            "batchnorm2           BatchNormalization   128                 \n",
            "relu2                ReLU                 0                   \n",
            "pool2                MaxPooling1D         0                   \n",
            "flatten              Flatten              0                   \n",
            "dense1               Dense                10272               \n",
            "batchnorm3           BatchNormalization   64                  \n",
            "relu3                ReLU                 0                   \n",
            "dropout2             Dropout              0                   \n",
            "dense2               Dense                33                  \n",
            "sigmoid1             Sigmoid              0                   \n",
            "------------------------------------------------------------\n",
            "Total parameters: 23617\n",
            "============================================================\n",
            "\n",
            "\n",
            "âœ“ Using weighted focal loss (gamma=1.0)\n",
            "  Class 0 weight: 1.3181 (boosted), Class 1 weight: 0.9177\n",
            "  Learning rate: 0.0002\n",
            "\n",
            "Training for up to 30 epochs...\n",
            "Early stopping based on validation accuracy (patience=5)\n",
            "Epoch 1/30 | Train: 50.1% | Val: 43.5% | Gap: +6.6% | Class0: 89.8% | Class1: 9.6%\n",
            "Epoch 3/30 | Train: 50.3% | Val: 43.1% | Gap: +7.2% | Class0: 92.7% | Class1: 6.7%\n",
            "Epoch 6/30 | Train: 51.0% | Val: 45.5% | Gap: +5.5% | Class0: 81.0% | Class1: 19.4%\n",
            "Epoch 9/30 | Train: 52.4% | Val: 44.2% | Gap: +8.3% | Class0: 86.3% | Class1: 13.3%\n",
            "\n",
            "Early stopping at epoch 11 (no improvement for 5 epochs)\n",
            "\n",
            "âœ“ Training completed!\n",
            "  Best Val Accuracy: 45.47% (Loss: 0.3926)\n",
            "  Final Val Accuracy: 43.88%\n",
            "  Train Accuracy: 47.47%\n",
            "  Class 0 (Lower) Accuracy: 92.61%\n",
            "  Class 1 (Higher) Accuracy: 8.18%\n",
            "âœ“ Memory freed after Conv1D training\n",
            "\n",
            "============================================================\n",
            "Training Hybrid (Conv1D + LSTM)...\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "Training Hybrid\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "Network: Hybrid_ConvLSTM_V2\n",
            "============================================================\n",
            "Layer Name           Type                 Parameters          \n",
            "------------------------------------------------------------\n",
            "conv1                Conv1D               6848                \n",
            "batchnorm1           BatchNormalization   64                  \n",
            "relu1                ReLU                 0                   \n",
            "pool1                MaxPooling1D         0                   \n",
            "dropout1             Dropout              0                   \n",
            "lstm1                LSTM                 8320                \n",
            "batchnorm2           BatchNormalization   64                  \n",
            "dropout2             Dropout              0                   \n",
            "dense1               Dense                528                 \n",
            "relu2                ReLU                 0                   \n",
            "dropout3             Dropout              0                   \n",
            "dense2               Dense                17                  \n",
            "sigmoid1             Sigmoid              0                   \n",
            "------------------------------------------------------------\n",
            "Total parameters: 15841\n",
            "============================================================\n",
            "\n",
            "\n",
            "âœ“ Using weighted focal loss (gamma=1.0)\n",
            "  Class 0 weight: 1.3181 (boosted), Class 1 weight: 0.9177\n",
            "  Learning rate: 0.0002\n",
            "\n",
            "Training for up to 30 epochs...\n",
            "Early stopping based on validation accuracy (patience=5)\n",
            "Epoch 1/30 | Train: 46.9% | Val: 42.6% | Gap: +4.3% | Class0: 97.5% | Class1: 2.4%\n",
            "Epoch 3/30 | Train: 47.4% | Val: 42.4% | Gap: +5.0% | Class0: 99.7% | Class1: 0.4%\n",
            "Epoch 6/30 | Train: 50.7% | Val: 48.4% | Gap: +2.4% | Class0: 78.7% | Class1: 26.1%\n",
            "Epoch 9/30 | Train: 54.3% | Val: 50.3% | Gap: +4.0% | Class0: 57.8% | Class1: 44.8%\n",
            "Epoch 12/30 | Train: 56.3% | Val: 53.5% | Gap: +2.8% | Class0: 50.0% | Class1: 56.2%\n",
            "Epoch 15/30 | Train: 57.2% | Val: 51.0% | Gap: +6.2% | Class0: 60.9% | Class1: 43.7%\n",
            "Epoch 18/30 | Train: 59.7% | Val: 54.6% | Gap: +5.1% | Class0: 47.4% | Class1: 59.8%\n",
            "Epoch 21/30 | Train: 59.5% | Val: 53.7% | Gap: +5.8% | Class0: 51.7% | Class1: 55.1%\n",
            "\n",
            "Early stopping at epoch 23 (no improvement for 5 epochs)\n",
            "\n",
            "âœ“ Training completed!\n",
            "  Best Val Accuracy: 54.57% (Loss: 0.4175)\n",
            "  Final Val Accuracy: 54.87%\n",
            "  Train Accuracy: 60.05%\n",
            "  Class 0 (Lower) Accuracy: 44.49%\n",
            "  Class 1 (Higher) Accuracy: 62.48%\n",
            "âœ“ Memory freed after Hybrid training\n",
            "\n",
            "============================================================\n",
            "All Models Trained!\n",
            "============================================================\n",
            "ðŸ† Best Model: Hybrid (Val Accuracy: 54.57%)\n"
          ]
        }
      ],
      "source": [
        "# Train all models - Ultra Memory-optimized version\n",
        "# Only keep results and best model, not all models in memory\n",
        "import gc\n",
        "import sys\n",
        "\n",
        "models_results = {}\n",
        "trained_models = {}\n",
        "best_model_name = None\n",
        "best_model = None\n",
        "best_trainer = None\n",
        "best_val_acc = 0.0\n",
        "\n",
        "# Train LSTM (with focal loss)\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Training LSTM...\")\n",
        "print(\"=\"*60)\n",
        "lstm_model, lstm_trainer, lstm_results = build_and_train_model(\n",
        "    \"LSTM\", build_lstm_model, X_train, y_train, X_val, y_val,\n",
        "    use_focal_loss=True\n",
        ")\n",
        "models_results['LSTM'] = lstm_results\n",
        "if lstm_results['val_accuracy'] > best_val_acc:\n",
        "    best_val_acc = lstm_results['val_accuracy']\n",
        "    best_model_name = 'LSTM'\n",
        "    best_model = lstm_model\n",
        "    best_trainer = lstm_trainer\n",
        "trained_models['LSTM'] = (lstm_model, lstm_trainer)\n",
        "del lstm_model, lstm_trainer\n",
        "gc.collect()\n",
        "print(\"âœ“ Memory freed after LSTM training\")\n",
        "\n",
        "# Train GRU (WITHOUT focal loss - prevents collapse, with higher LR)\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Training GRU (no focal loss for stability)...\")\n",
        "print(\"=\"*60)\n",
        "gc.collect()\n",
        "gru_model, gru_trainer, gru_results = build_and_train_model(\n",
        "    \"GRU\", build_gru_model, X_train, y_train, X_val, y_val,\n",
        "    use_focal_loss=False,  # Disable focal loss to prevent collapse\n",
        "    custom_lr=0.0005  # Higher LR for GRU to escape local minima\n",
        ")\n",
        "models_results['GRU'] = gru_results\n",
        "if gru_results['val_accuracy'] > best_val_acc:\n",
        "    best_val_acc = gru_results['val_accuracy']\n",
        "    best_model_name = 'GRU'\n",
        "    best_model = gru_model\n",
        "    best_trainer = gru_trainer\n",
        "trained_models['GRU'] = (gru_model, gru_trainer)\n",
        "del gru_model, gru_trainer\n",
        "gc.collect()\n",
        "print(\"âœ“ Memory freed after GRU training\")\n",
        "\n",
        "# Train Conv1D\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Training Conv1D...\")\n",
        "print(\"=\"*60)\n",
        "gc.collect()\n",
        "conv_model, conv_trainer, conv_results = build_and_train_model(\n",
        "    \"Conv1D\", build_conv1d_model, X_train, y_train, X_val, y_val,\n",
        "    use_focal_loss=True\n",
        ")\n",
        "models_results['Conv1D'] = conv_results\n",
        "if conv_results['val_accuracy'] > best_val_acc:\n",
        "    best_val_acc = conv_results['val_accuracy']\n",
        "    best_model_name = 'Conv1D'\n",
        "    best_model = conv_model\n",
        "    best_trainer = conv_trainer\n",
        "trained_models['Conv1D'] = (conv_model, conv_trainer)\n",
        "del conv_model, conv_trainer\n",
        "gc.collect()\n",
        "print(\"âœ“ Memory freed after Conv1D training\")\n",
        "\n",
        "# Train Hybrid (Conv1D + LSTM)\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Training Hybrid (Conv1D + LSTM)...\")\n",
        "print(\"=\"*60)\n",
        "gc.collect()\n",
        "hybrid_model, hybrid_trainer, hybrid_results = build_and_train_model(\n",
        "    \"Hybrid\", build_hybrid_model, X_train, y_train, X_val, y_val,\n",
        "    use_focal_loss=True\n",
        ")\n",
        "models_results['Hybrid'] = hybrid_results\n",
        "if hybrid_results['val_accuracy'] > best_val_acc:\n",
        "    best_val_acc = hybrid_results['val_accuracy']\n",
        "    best_model_name = 'Hybrid'\n",
        "    best_model = hybrid_model\n",
        "    best_trainer = hybrid_trainer\n",
        "trained_models['Hybrid'] = (hybrid_model, hybrid_trainer)\n",
        "del hybrid_model, hybrid_trainer\n",
        "gc.collect()\n",
        "print(\"âœ“ Memory freed after Hybrid training\")\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"All Models Trained!\")\n",
        "print(f\"{'='*60}\")\n",
        "print(f\"ðŸ† Best Model: {best_model_name} (Val Accuracy: {best_val_acc:.2f}%)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Compare Models and Select Best\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "Model Comparison\n",
            "============================================================\n",
            "\n",
            " Model  Val Loss  Val Accuracy  Train Accuracy  Class 0 Acc  Class 1 Acc   Balance\n",
            "Hybrid  0.417458     54.573333       60.052000    44.489989    62.481229 82.008760\n",
            "  LSTM  0.440989     53.980000       69.881333    32.933943    71.421971 61.511972\n",
            "Conv1D  0.392557     45.466667       47.466667    92.606022     8.178353 15.572330\n",
            "   GRU  0.770712     43.326667       51.368000    97.319880     3.765739  6.445859\n",
            "\n",
            "============================================================\n",
            "ðŸ† Best Model (by Val Accuracy): Hybrid\n",
            "============================================================\n",
            "  Validation Loss: 0.4175\n",
            "  Validation Accuracy: 54.57%\n",
            "  Train Accuracy: 60.05%\n",
            "  Generalization Gap: 5.48%\n",
            "\n",
            "============================================================\n",
            "Detailed Evaluation: Hybrid\n",
            "============================================================\n",
            "\n",
            "Metrics:\n",
            "  Precision: 0.6057\n",
            "  Recall: 0.6248\n",
            "  F1-Score: 0.6151\n",
            "\n",
            "Confusion Matrix:\n",
            "                Predicted\n",
            "              â†“ (0)   â†‘ (1)\n",
            "Actual â†“ (0)    2822    3521\n",
            "       â†‘ (1)    3248    5409\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     â†“ Lower       0.46      0.44      0.45      6343\n",
            "    â†‘ Higher       0.61      0.62      0.62      8657\n",
            "\n",
            "    accuracy                           0.55     15000\n",
            "   macro avg       0.54      0.53      0.53     15000\n",
            "weighted avg       0.55      0.55      0.55     15000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Compare all models - Select best by validation accuracy (better for classification)\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Model Comparison\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "comparison_df = pd.DataFrame([\n",
        "    {\n",
        "        'Model': name,\n",
        "        'Val Loss': results['val_loss'],\n",
        "        'Val Accuracy': results['val_accuracy'],\n",
        "        'Train Accuracy': results['train_accuracy'],\n",
        "        'Class 0 Acc': results.get('class_0_accuracy', 0),\n",
        "        'Class 1 Acc': results.get('class_1_accuracy', 0)\n",
        "    }\n",
        "    for name, results in models_results.items()\n",
        "]).sort_values('Val Accuracy', ascending=False)  # Sort by accuracy (descending)\n",
        "\n",
        "# Calculate balance score (how well balanced are predictions between classes)\n",
        "comparison_df['Balance'] = 100 - abs(comparison_df['Class 0 Acc'] - comparison_df['Class 1 Acc'])\n",
        "\n",
        "print(\"\\n\" + comparison_df.to_string(index=False))\n",
        "\n",
        "# Select best model by validation accuracy (more meaningful for classification)\n",
        "# Override the loss-based selection with accuracy-based selection\n",
        "best_model_name_by_accuracy = comparison_df.iloc[0]['Model']\n",
        "best_model_by_accuracy, best_trainer_by_accuracy = trained_models[best_model_name_by_accuracy]\n",
        "\n",
        "# Use accuracy-based selection\n",
        "best_model_name = best_model_name_by_accuracy\n",
        "best_model = best_model_by_accuracy\n",
        "best_trainer = best_trainer_by_accuracy\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"ðŸ† Best Model (by Val Accuracy): {best_model_name}\")\n",
        "print(f\"{'='*60}\")\n",
        "best_row = comparison_df[comparison_df['Model'] == best_model_name].iloc[0]\n",
        "print(f\"  Validation Loss: {best_row['Val Loss']:.4f}\")\n",
        "print(f\"  Validation Accuracy: {best_row['Val Accuracy']:.2f}%\")\n",
        "print(f\"  Train Accuracy: {best_row['Train Accuracy']:.2f}%\")\n",
        "print(f\"  Generalization Gap: {best_row['Train Accuracy'] - best_row['Val Accuracy']:.2f}%\")\n",
        "\n",
        "# Detailed evaluation of best model\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"Detailed Evaluation: {best_model_name}\")\n",
        "print(f\"{'='*60}\")\n",
        "\n",
        "# Make predictions\n",
        "y_pred = best_trainer.predict(X_val)\n",
        "y_pred_binary = (y_pred > 0.5).astype(int).flatten()\n",
        "\n",
        "# Calculate metrics\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
        "\n",
        "precision = precision_score(y_val, y_pred_binary, zero_division=0)\n",
        "recall = recall_score(y_val, y_pred_binary, zero_division=0)\n",
        "f1 = f1_score(y_val, y_pred_binary, zero_division=0)\n",
        "\n",
        "print(f\"\\nMetrics:\")\n",
        "print(f\"  Precision: {precision:.4f}\")\n",
        "print(f\"  Recall: {recall:.4f}\")\n",
        "print(f\"  F1-Score: {f1:.4f}\")\n",
        "\n",
        "print(f\"\\nConfusion Matrix:\")\n",
        "cm = confusion_matrix(y_val, y_pred_binary)\n",
        "print(f\"                Predicted\")\n",
        "print(f\"              â†“ (0)   â†‘ (1)\")\n",
        "print(f\"Actual â†“ (0)  {cm[0,0]:6d}  {cm[0,1]:6d}\")\n",
        "print(f\"       â†‘ (1)  {cm[1,0]:6d}  {cm[1,1]:6d}\")\n",
        "\n",
        "print(f\"\\nClassification Report:\")\n",
        "print(classification_report(y_val, y_pred_binary, target_names=['â†“ Lower', 'â†‘ Higher']))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Save Models\n",
        "\n",
        "Save all trained models and the best model for later use.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Summary\n",
        "\n",
        "Training complete! The best model has been selected and saved. Next steps:\n",
        "1. Use `04_evaluation.ipynb` for detailed evaluation and visualization\n",
        "2. Use `05_submission.ipynb` to generate Kaggle predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Saving models to ../models...\n",
            "âœ“ Saved best model (Hybrid) parameters\n",
            "âœ“ Model comparison saved\n",
            "\n",
            "âœ“ Models saved!\n",
            "âœ“ Best model: Hybrid\n"
          ]
        }
      ],
      "source": [
        "# Save models - Only save best model to save disk space\n",
        "model_dir = Path(\"../models\")\n",
        "model_dir.mkdir(exist_ok=True)\n",
        "\n",
        "print(f\"\\nSaving models to {model_dir}...\")\n",
        "\n",
        "# Only save best model (saves disk space)\n",
        "best_params = best_model.get_params()\n",
        "with open(model_dir / \"best_model_params.pkl\", \"wb\") as f:\n",
        "    pickle.dump(best_params, f)\n",
        "print(f\"âœ“ Saved best model ({best_model_name}) parameters\")\n",
        "\n",
        "# Save model comparison results (small file)\n",
        "with open(model_dir / \"model_comparison.pkl\", \"wb\") as f:\n",
        "    pickle.dump({\n",
        "        'comparison': comparison_df.to_dict('records'),\n",
        "        'best_model': best_model_name,\n",
        "        'models_results': models_results,\n",
        "        'sequence_length': SEQUENCE_LENGTH,\n",
        "        'feature_cols': feature_cols\n",
        "    }, f)\n",
        "print(f\"âœ“ Model comparison saved\")\n",
        "\n",
        "# Optionally save other models (commented out to save disk space)\n",
        "# Uncomment if you need all models saved\n",
        "# for model_name, (model, trainer) in trained_models.items():\n",
        "#     if model_name != best_model_name:  # Skip best model (already saved)\n",
        "#         params = model.get_params()\n",
        "#         with open(model_dir / f\"{model_name.lower()}_params.pkl\", \"wb\") as f:\n",
        "#             pickle.dump(params, f)\n",
        "#         print(f\"âœ“ Saved {model_name} parameters\")\n",
        "\n",
        "print(f\"\\nâœ“ Models saved!\")\n",
        "print(f\"âœ“ Best model: {best_model_name}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
