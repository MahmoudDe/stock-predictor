{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import pickle\n",
        "import importlib\n",
        "\n",
        "sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath('__file__'))))\n",
        "\n",
        "import KhayatMiniNN.optimizers.adam as adam_module\n",
        "importlib.reload(adam_module)\n",
        "\n",
        "from KhayatMiniNN.neural_network import NeuralNetwork\n",
        "from KhayatMiniNN.trainer import Trainer\n",
        "from KhayatMiniNN.layers import LSTM, GRU, Conv1D, Dense, ReLU, Sigmoid, MaxPooling1D\n",
        "from KhayatMiniNN.layers.base import Layer\n",
        "from KhayatMiniNN.regularization import Dropout, BatchNormalization\n",
        "from KhayatMiniNN.losses import BinaryCrossEntropy\n",
        "from KhayatMiniNN.optimizers.adam import Adam  # Import directly from reloaded module\n",
        "from KhayatMiniNN.utils.sequence_utils import create_sequences\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "class WeightedBinaryCrossEntropy:\n",
        "\n",
        "    def __init__(self, weight_for_0, weight_for_1, from_logits=False, focal_gamma=0.0):\n",
        "        self.base_loss = BinaryCrossEntropy(from_logits=from_logits)\n",
        "        self.weight_for_0 = weight_for_0\n",
        "        self.weight_for_1 = weight_for_1\n",
        "        self.from_logits = from_logits\n",
        "        self.focal_gamma = focal_gamma\n",
        "        self.predictions = None\n",
        "        self.targets = None\n",
        "        self.weights = None\n",
        "    \n",
        "    def forward(self, predictions, targets):\n",
        "        self.predictions = predictions\n",
        "        self.targets = targets\n",
        "        \n",
        "        # Ensure correct shapes\n",
        "        if targets.ndim == 1:\n",
        "            targets_reshaped = targets.reshape(-1, 1)\n",
        "        else:\n",
        "            targets_reshaped = targets\n",
        "        \n",
        "        if predictions.ndim == 1:\n",
        "            predictions_reshaped = predictions.reshape(-1, 1)\n",
        "        else:\n",
        "            predictions_reshaped = predictions\n",
        "        \n",
        "        # Clip predictions to avoid log(0)\n",
        "        eps = 1e-7\n",
        "        predictions_clipped = np.clip(predictions_reshaped, eps, 1 - eps)\n",
        "        \n",
        "        # Calculate per-sample BCE loss\n",
        "        bce = -(targets_reshaped * np.log(predictions_clipped) + \n",
        "                (1 - targets_reshaped) * np.log(1 - predictions_clipped))\n",
        "        \n",
        "        # Calculate per-sample weights\n",
        "        self.weights = np.where(targets_reshaped == 0, self.weight_for_0, self.weight_for_1)\n",
        "        \n",
        "        # Apply focal loss modulation if gamma > 0\n",
        "        if self.focal_gamma > 0:\n",
        "            # pt = p if y=1, else (1-p)\n",
        "            pt = np.where(targets_reshaped == 1, predictions_clipped, 1 - predictions_clipped)\n",
        "            focal_weight = (1 - pt) ** self.focal_gamma\n",
        "            bce = focal_weight * bce\n",
        "        \n",
        "        # Apply class weights per sample\n",
        "        weighted_bce = self.weights * bce\n",
        "        \n",
        "        return np.mean(weighted_bce)\n",
        "    \n",
        "    def backward(self):\n",
        "        eps = 1e-7\n",
        "        predictions = self.predictions\n",
        "        targets = self.targets\n",
        "        \n",
        "        if targets.ndim == 1:\n",
        "            targets = targets.reshape(-1, 1)\n",
        "        if predictions.ndim == 1:\n",
        "            predictions = predictions.reshape(-1, 1)\n",
        "        \n",
        "        predictions_clipped = np.clip(predictions, eps, 1 - eps)\n",
        "        \n",
        "        grad = (predictions_clipped - targets) / (predictions_clipped * (1 - predictions_clipped) + eps)\n",
        "        \n",
        "        weighted_grad = self.weights * grad / targets.shape[0]\n",
        "        \n",
        "        return weighted_grad\n",
        "    \n",
        "    def set_loss(self, loss_fn):\n",
        "        pass\n",
        "\n",
        "class Flatten(Layer):\n",
        "    def __init__(self, name=\"Flatten\"):\n",
        "        super().__init__(name)\n",
        "    \n",
        "    def forward(self, input_data):\n",
        "        self.input = input_data\n",
        "        batch_size = input_data.shape[0]\n",
        "        return input_data.reshape(batch_size, -1)\n",
        "    \n",
        "    def backward(self, output_grad):\n",
        "        return output_grad.reshape(self.input.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train: 5,505,350 samples. Val: 5,475,700. Features: 71.\n"
          ]
        }
      ],
      "source": [
        "data_dir = Path(\"../data/processed\")\n",
        "train_df = pd.read_csv(data_dir / \"train_features.csv\")\n",
        "val_df = pd.read_csv(data_dir / \"val_features.csv\")\n",
        "\n",
        "with open(data_dir / \"feature_cols.pkl\", \"rb\") as f:\n",
        "    feature_cols = pickle.load(f)\n",
        "\n",
        "\n",
        "missing = [c for c in feature_cols if c not in train_df.columns]\n",
        "if missing:\n",
        "    raise KeyError(\n",
        "        + str(missing[:10])\n",
        "        + \". Re-run the feature engineering notebook (2_feature_engineering) to regenerate the processed files.\"\n",
        "    )\n",
        "\n",
        "print(f\"Train: {len(train_df):,} samples. Val: {len(val_df):,}. Features: {len(feature_cols)}.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create Sequences\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "Creating Sequences\n",
            "============================================================\n",
            "\n",
            "Using MAX_SAMPLES = 150,000 (‚âà1.9 GB)\n",
            "\n",
            "Creating sequences for train set...\n",
            "  Creating sequences per ticker...\n",
            "  Completed: 150,000 sequences created\n",
            "Train sequences: (150000, 20, 71)\n",
            "Memory usage: 0.79 GB (float32)\n",
            "\n",
            "Creating sequences for validation set...\n",
            "  Creating sequences per ticker...\n",
            "  Completed: 15,000 sequences created\n",
            "Val sequences: (15000, 20, 71)\n",
            "Memory usage: 0.08 GB (float32)\n",
            "\n",
            "Class weights (for imbalanced data):\n",
            "  Class 0 (Lower): 1.0985\n",
            "  Class 1 (Higher): 0.9177\n",
            "\n",
            "Target distribution (train):\n",
            "(Higher): 81722 (54.48%)\n",
            "(Lower): 68278 (45.52%)\n",
            "\n",
            "Target distribution (val):\n",
            "(Higher): 8657 (57.71%)\n",
            "(Lower): 6343 (42.29%)\n",
            "Dataset limited to 150,000 samples per set.\n",
            "\n",
            "‚úì Freed memory from dataframes\n"
          ]
        }
      ],
      "source": [
        "SEQUENCE_LENGTH = 20 \n",
        "BATCH_SIZE = 64 \n",
        "EPOCHS = 30\n",
        "LEARNING_RATE = 0.0002\n",
        "EARLY_STOPPING_PATIENCE = 5\n",
        "\n",
        "MAX_SAMPLES = 150000\n",
        "SAMPLE_RATE = 1.0\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "def create_sequences_in_memory(df, feature_cols, sequence_length, target_col='target', \n",
        "                               max_samples=None, sample_rate=1.0):\n",
        "  \n",
        "    if max_samples is None:\n",
        "        raise ValueError(\"max_samples must be set to avoid memory errors. Try MAX_SAMPLES = 500000 or adjust based on available RAM.\")\n",
        "    \n",
        "    X = np.zeros((max_samples, sequence_length, len(feature_cols)), dtype=np.float32)\n",
        "    y = np.zeros(max_samples, dtype=np.float32)\n",
        "    current_idx = 0\n",
        "    \n",
        "    print(\"  Creating sequences per ticker...\")\n",
        "    tickers = df['Ticker'].unique()\n",
        "    \n",
        "    for ticker_idx, ticker in enumerate(tickers):\n",
        "        if current_idx >= max_samples:\n",
        "            break\n",
        "            \n",
        "        ticker_data = df[df['Ticker'] == ticker].sort_values('Date')\n",
        "        features = ticker_data[feature_cols].values.astype(np.float32)\n",
        "        targets = ticker_data[target_col].values.astype(np.float32)\n",
        "        \n",
        "        if len(features) < sequence_length:\n",
        "            continue\n",
        "        \n",
        "        # Create sliding window sequences for this ticker\n",
        "        num_sequences = len(features) - sequence_length + 1\n",
        "        \n",
        "        # Apply sampling if needed\n",
        "        if sample_rate < 1.0:\n",
        "            num_sequences = int(num_sequences * sample_rate)\n",
        "            indices = np.random.choice(len(features) - sequence_length + 1, \n",
        "                                      size=num_sequences, replace=False)\n",
        "            indices = np.sort(indices)\n",
        "        else:\n",
        "            indices = np.arange(len(features) - sequence_length + 1)\n",
        "        \n",
        "        for i in indices:\n",
        "            if current_idx >= max_samples:\n",
        "                break\n",
        "            X[current_idx] = features[i:i+sequence_length]\n",
        "            y[current_idx] = targets[i+sequence_length-1]\n",
        "            current_idx += 1\n",
        "        \n",
        "        if (ticker_idx + 1) % 100 == 0:\n",
        "            print(f\"  Processed {ticker_idx + 1}/{len(tickers)} tickers, {current_idx:,} sequences...\", end='\\r')\n",
        "    \n",
        "    print(f\"  Completed: {current_idx:,} sequences created\")\n",
        "    \n",
        "    # Trim to actual size\n",
        "    if current_idx < max_samples:\n",
        "        X = X[:current_idx]\n",
        "        y = y[:current_idx]\n",
        "    \n",
        "    return X, y\n",
        "\n",
        "seq_dir = Path(\"../data/sequences\")\n",
        "if seq_dir.exists():\n",
        "    print(\"Cleaning up existing sequence files to free disk space...\")\n",
        "    for file in seq_dir.glob(\"*.dat\"):\n",
        "        try:\n",
        "            file.unlink()\n",
        "            print(f\"  Deleted: {file.name}\")\n",
        "        except Exception as e:\n",
        "            print(f\"  Could not delete {file.name}: {e}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"Creating Sequences\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"\\nUsing MAX_SAMPLES = {MAX_SAMPLES:,} (‚âà{MAX_SAMPLES * 30 * 115 * 4 / 1024**3:.1f} GB)\")\n",
        "\n",
        "print(\"\\nCreating sequences for train set...\")\n",
        "X_train, y_train = create_sequences_in_memory(\n",
        "    train_df, feature_cols, SEQUENCE_LENGTH, \n",
        "    max_samples=MAX_SAMPLES, sample_rate=SAMPLE_RATE\n",
        ")\n",
        "print(f\"Train sequences: {X_train.shape}\")\n",
        "print(f\"Memory usage: {X_train.nbytes / 1024**3:.2f} GB (float32)\")\n",
        "\n",
        "print(\"\\nCreating sequences for validation set...\")\n",
        "# Use same sampling for validation, but limit to reasonable size\n",
        "val_max_samples = MAX_SAMPLES // 10 if MAX_SAMPLES else None  # 10% of train size\n",
        "X_val, y_val = create_sequences_in_memory(\n",
        "    val_df, feature_cols, SEQUENCE_LENGTH,\n",
        "    max_samples=val_max_samples, sample_rate=SAMPLE_RATE\n",
        ")\n",
        "print(f\"Val sequences: {X_val.shape}\")\n",
        "print(f\"Memory usage: {X_val.nbytes / 1024**3:.2f} GB (float32)\")\n",
        "\n",
        "# Calculate class weights for imbalanced data\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "# Calculate class weights\n",
        "classes = np.unique(y_train)\n",
        "class_weights = compute_class_weight('balanced', classes=classes, y=y_train)\n",
        "class_weight_dict = dict(zip(classes, class_weights))\n",
        "\n",
        "print(f\"\\nClass weights (for imbalanced data):\")\n",
        "print(f\"  Class 0 (Lower): {class_weight_dict[0]:.4f}\")\n",
        "print(f\"  Class 1 (Higher): {class_weight_dict[1]:.4f}\")\n",
        "\n",
        "# Store for use in training\n",
        "weight_for_0 = class_weight_dict[0]\n",
        "weight_for_1 = class_weight_dict[1]\n",
        "\n",
        "print(f\"\\nTarget distribution (train):\")\n",
        "print(f\"(Higher): {y_train.sum():.0f} ({y_train.mean()*100:.2f}%)\")\n",
        "print(f\"(Lower): {(len(y_train) - y_train.sum()):.0f} ({(1-y_train.mean())*100:.2f}%)\")\n",
        "\n",
        "print(f\"\\nTarget distribution (val):\")\n",
        "print(f\"(Higher): {y_val.sum():.0f} ({y_val.mean()*100:.2f}%)\")\n",
        "print(f\"(Lower): {(len(y_val) - y_val.sum()):.0f} ({(1-y_val.mean())*100:.2f}%)\")\n",
        "\n",
        "if MAX_SAMPLES:\n",
        "    print(f\"Dataset limited to {MAX_SAMPLES:,} samples per set.\")\n",
        "if SAMPLE_RATE < 1.0:\n",
        "    print(f\"Using {SAMPLE_RATE*100:.1f}% of available sequences.\")\n",
        "\n",
        "# Free up memory by deleting dataframes (sequences are already created)\n",
        "del train_df, val_df\n",
        "import gc\n",
        "gc.collect()\n",
        "print(\"\\n‚úì Freed memory from dataframes\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Build Multiple Model Architectures\n",
        "\n",
        "We'll create and train multiple models:\n",
        "1. **LSTM** - Long Short-Term Memory\n",
        "2. **GRU** - Gated Recurrent Unit  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Model architectures defined. Ready to train!\n"
          ]
        }
      ],
      "source": [
        "input_size = len(feature_cols)\n",
        "hidden_size = 64\n",
        "lstm_hidden = 32\n",
        "gru_hidden = 48\n",
        "conv_filters = 32\n",
        "\n",
        "\n",
        "FOCAL_GAMMA = 1.0\n",
        "CLASS_WEIGHT_BOOST = 1.2 \n",
        "\n",
        "L2_WEIGHT_DECAY = 0.001\n",
        "\n",
        "def build_and_train_model(model_name, architecture_func, X_train, y_train, X_val, y_val, \n",
        "                         use_weighted_loss=True, use_focal_loss=True, custom_lr=None):\n",
        "    \n",
        "    import gc\n",
        "    import sys\n",
        "    \n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Training {model_name}\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    # Build model\n",
        "    model = architecture_func()\n",
        "    model.summary()\n",
        "    \n",
        "    # Set loss - use weighted loss with optional focal loss\n",
        "    if use_weighted_loss and 'weight_for_0' in globals() and 'weight_for_1' in globals():\n",
        "        # Boost the minority class weight to balance predictions\n",
        "        boosted_weight_0 = weight_for_0 * CLASS_WEIGHT_BOOST\n",
        "        focal_gamma = FOCAL_GAMMA if use_focal_loss else 0.0\n",
        "        loss_fn = WeightedBinaryCrossEntropy(\n",
        "            weight_for_0=boosted_weight_0, \n",
        "            weight_for_1=weight_for_1, \n",
        "            from_logits=False,\n",
        "            focal_gamma=focal_gamma\n",
        "        )\n",
        "        if use_focal_loss:\n",
        "            print(f\"\\n‚úì Using weighted focal loss (gamma={focal_gamma})\")\n",
        "        else:\n",
        "            print(f\"\\n‚úì Using weighted BCE (no focal loss - more stable for some architectures)\")\n",
        "        print(f\"  Class 0 weight: {boosted_weight_0:.4f} (boosted), Class 1 weight: {weight_for_1:.4f}\")\n",
        "    else:\n",
        "        loss_fn = BinaryCrossEntropy(from_logits=False)\n",
        "        print(f\"\\n‚ö† Using standard loss (no class weights)\")\n",
        "    \n",
        "    model.set_loss(loss_fn)\n",
        "    \n",
        "    lr = custom_lr if custom_lr is not None else LEARNING_RATE\n",
        "    optimizer = Adam(learning_rate=lr, weight_decay=L2_WEIGHT_DECAY)\n",
        "    trainer = Trainer(model, optimizer, loss_fn)\n",
        "    print(f\"  Learning rate: {lr}\")\n",
        "    \n",
        "    best_val_acc = 0.0\n",
        "    best_val_loss = float('inf')\n",
        "    patience_counter = 0\n",
        "    best_params = None\n",
        "    \n",
        "    print(f\"\\nTraining for up to {EPOCHS} epochs...\")\n",
        "    print(f\"Early stopping based on validation accuracy (patience={EARLY_STOPPING_PATIENCE})\")\n",
        "    \n",
        "    for epoch in range(EPOCHS):\n",
        "        # Aggressive memory cleanup before each epoch\n",
        "        gc.collect()\n",
        "        \n",
        "        # Train one epoch\n",
        "        history = trainer.fit(\n",
        "            X_train, y_train.reshape(-1, 1),\n",
        "            X_val=X_val, y_val=y_val.reshape(-1, 1),\n",
        "            epochs=1,\n",
        "            batch_size=BATCH_SIZE,\n",
        "            verbose=False\n",
        "        )\n",
        "        \n",
        "        # Get validation metrics\n",
        "        val_loss, val_acc = trainer.evaluate(X_val, y_val.reshape(-1, 1))\n",
        "        \n",
        "        # Calculate class-specific metrics every few epochs\n",
        "        if (epoch + 1) % 3 == 0 or epoch == 0:\n",
        "            train_loss, train_acc = trainer.evaluate(X_train, y_train.reshape(-1, 1))\n",
        "            \n",
        "            # Get predictions for class-specific metrics\n",
        "            y_val_pred = (trainer.predict(X_val) > 0.5).astype(int).flatten()\n",
        "            class_0_acc = np.mean(y_val_pred[y_val == 0] == 0) * 100 if np.sum(y_val == 0) > 0 else 0\n",
        "            class_1_acc = np.mean(y_val_pred[y_val == 1] == 1) * 100 if np.sum(y_val == 1) > 0 else 0\n",
        "            \n",
        "            print(f\"Epoch {epoch+1}/{EPOCHS} | \"\n",
        "                  f\"Train: {train_acc:.1f}% | Val: {val_acc:.1f}% | \"\n",
        "                  f\"Gap: {train_acc - val_acc:+.1f}% | \"\n",
        "                  f\"Class0: {class_0_acc:.1f}% | Class1: {class_1_acc:.1f}%\")\n",
        "        \n",
        "        # Early stopping based on validation accuracy (better than loss for classification)\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            best_val_loss = val_loss\n",
        "            patience_counter = 0\n",
        "            # Store best params\n",
        "            best_params = model.get_params()\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= EARLY_STOPPING_PATIENCE:\n",
        "                print(f\"\\nEarly stopping at epoch {epoch+1} (no improvement for {EARLY_STOPPING_PATIENCE} epochs)\")\n",
        "                break\n",
        "        \n",
        "        gc.collect()\n",
        "    \n",
        "    # Restore best parameters\n",
        "    if best_params:\n",
        "        model.set_params(best_params)\n",
        "    \n",
        "    # Final evaluation\n",
        "    val_loss, val_accuracy = trainer.evaluate(X_val, y_val.reshape(-1, 1))\n",
        "    train_loss, train_accuracy = trainer.evaluate(X_train, y_train.reshape(-1, 1))\n",
        "    \n",
        "    # Final class-specific metrics\n",
        "    y_val_pred = (trainer.predict(X_val) > 0.5).astype(int).flatten()\n",
        "    final_class_0_acc = np.mean(y_val_pred[y_val == 0] == 0) * 100 if np.sum(y_val == 0) > 0 else 0\n",
        "    final_class_1_acc = np.mean(y_val_pred[y_val == 1] == 1) * 100 if np.sum(y_val == 1) > 0 else 0\n",
        "    \n",
        "    print(f\"\\n‚úì Training completed!\")\n",
        "    print(f\"  Best Val Accuracy: {best_val_acc:.2f}% (Loss: {best_val_loss:.4f})\")\n",
        "    print(f\"  Final Val Accuracy: {val_accuracy:.2f}%\")\n",
        "    print(f\"  Train Accuracy: {train_accuracy:.2f}%\")\n",
        "    print(f\"  Class 0 (Lower) Accuracy: {final_class_0_acc:.2f}%\")\n",
        "    print(f\"  Class 1 (Higher) Accuracy: {final_class_1_acc:.2f}%\")\n",
        "    \n",
        "    # Clear trainer optimizer states to free memory\n",
        "    if hasattr(trainer, 'layer_optimizers'):\n",
        "        trainer.layer_optimizers.clear()\n",
        "    \n",
        "    return model, trainer, {\n",
        "        'name': model_name,\n",
        "        'val_loss': best_val_loss,\n",
        "        'val_accuracy': best_val_acc,\n",
        "        'train_accuracy': train_accuracy,\n",
        "        'class_0_accuracy': final_class_0_acc,\n",
        "        'class_1_accuracy': final_class_1_acc,\n",
        "        'history': history\n",
        "    }\n",
        "\n",
        "# Define model architectures - TUNED FOR BETTER RESULTS\n",
        "def build_lstm_model():\n",
        "    model = NeuralNetwork(name=\"LSTM_Stock_Predictor_V2\")\n",
        "    model.add_layer(LSTM(input_size, lstm_hidden, return_sequences=False), name=\"lstm1\")\n",
        "    # Batch normalization for stable training\n",
        "    model.add_layer(BatchNormalization(lstm_hidden), name=\"batchnorm1\")\n",
        "    # Higher dropout to aggressively prevent overfitting\n",
        "    model.add_layer(Dropout(dropout_rate=0.6), name=\"dropout1\")\n",
        "    # First dense layer - smaller size for simpler model\n",
        "    model.add_layer(Dense(lstm_hidden, 16), name=\"dense1\")\n",
        "    model.add_layer(BatchNormalization(16), name=\"batchnorm2\")\n",
        "    model.add_layer(ReLU(), name=\"relu1\")\n",
        "    # Strong dropout before output\n",
        "    model.add_layer(Dropout(dropout_rate=0.5), name=\"dropout2\")\n",
        "    # Output layer\n",
        "    model.add_layer(Dense(16, 1), name=\"dense2\")\n",
        "    model.add_layer(Sigmoid(), name=\"sigmoid1\")\n",
        "    return model\n",
        "\n",
        "def build_gru_model():\n",
        "    model = NeuralNetwork(name=\"GRU_Stock_Predictor_V3\")\n",
        "    # GRU layer - simpler, larger hidden size for better gradient flow\n",
        "    model.add_layer(GRU(input_size, 64, return_sequences=False), name=\"gru1\")\n",
        "    # Lower dropout to prevent gradient starvation\n",
        "    model.add_layer(Dropout(dropout_rate=0.4), name=\"dropout1\")\n",
        "    # Direct to output with one hidden layer - simpler architecture\n",
        "    model.add_layer(Dense(64, 32), name=\"dense1\")\n",
        "    model.add_layer(ReLU(), name=\"relu1\")\n",
        "    model.add_layer(Dropout(dropout_rate=0.3), name=\"dropout2\")\n",
        "    model.add_layer(Dense(32, 1), name=\"dense2\")\n",
        "    model.add_layer(Sigmoid(), name=\"sigmoid1\")\n",
        "    return model\n",
        "\n",
        "def build_conv1d_model():\n",
        "    seq_after_pool1 = (SEQUENCE_LENGTH - 2) // 2 + 1\n",
        "    seq_after_pool2 = (seq_after_pool1 - 2) // 2 + 1\n",
        "    flattened_size = seq_after_pool2 * (conv_filters * 2)\n",
        "    \n",
        "    model = NeuralNetwork(name=\"Conv1D_Stock_Predictor_V2\")\n",
        "    model.add_layer(Conv1D(input_size, conv_filters, kernel_size=3, padding='same'), name=\"conv1\")\n",
        "    model.add_layer(BatchNormalization(conv_filters), name=\"batchnorm1\")\n",
        "    model.add_layer(ReLU(), name=\"relu1\")\n",
        "    model.add_layer(MaxPooling1D(pool_size=2, stride=2), name=\"pool1\")\n",
        "    model.add_layer(Dropout(dropout_rate=0.3), name=\"dropout1\")\n",
        "    model.add_layer(Conv1D(conv_filters, conv_filters*2, kernel_size=3, padding='same'), name=\"conv2\")\n",
        "    model.add_layer(BatchNormalization(conv_filters*2), name=\"batchnorm2\")\n",
        "    model.add_layer(ReLU(), name=\"relu2\")\n",
        "    model.add_layer(MaxPooling1D(pool_size=2, stride=2), name=\"pool2\")\n",
        "    model.add_layer(Flatten(), name=\"flatten\")\n",
        "    model.add_layer(Dense(flattened_size, 32), name=\"dense1\")\n",
        "    model.add_layer(BatchNormalization(32), name=\"batchnorm3\")\n",
        "    model.add_layer(ReLU(), name=\"relu3\")\n",
        "    model.add_layer(Dropout(dropout_rate=0.5), name=\"dropout2\")\n",
        "    model.add_layer(Dense(32, 1), name=\"dense2\")\n",
        "    model.add_layer(Sigmoid(), name=\"sigmoid1\")\n",
        "    return model\n",
        "\n",
        "def build_hybrid_model():\n",
        "    model = NeuralNetwork(name=\"Hybrid_ConvLSTM_V2\")\n",
        "    model.add_layer(Conv1D(input_size, conv_filters, kernel_size=3, padding='same'), name=\"conv1\")\n",
        "    model.add_layer(BatchNormalization(conv_filters), name=\"batchnorm1\")\n",
        "    model.add_layer(ReLU(), name=\"relu1\")\n",
        "    model.add_layer(MaxPooling1D(pool_size=2, stride=2), name=\"pool1\")\n",
        "    model.add_layer(Dropout(dropout_rate=0.3), name=\"dropout1\")\n",
        "    model.add_layer(LSTM(conv_filters, lstm_hidden, return_sequences=False), name=\"lstm1\")\n",
        "    model.add_layer(BatchNormalization(lstm_hidden), name=\"batchnorm2\")\n",
        "    model.add_layer(Dropout(dropout_rate=0.6), name=\"dropout2\")\n",
        "    model.add_layer(Dense(lstm_hidden, 16), name=\"dense1\")\n",
        "    model.add_layer(ReLU(), name=\"relu2\")\n",
        "    model.add_layer(Dropout(dropout_rate=0.5), name=\"dropout3\")\n",
        "    model.add_layer(Dense(16, 1), name=\"dense2\")\n",
        "    model.add_layer(Sigmoid(), name=\"sigmoid1\")\n",
        "    return model\n",
        "\n",
        "print(\"\\nModel architectures defined. Ready to train!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Train All Models\n",
        "\n",
        "We'll train all model architectures and compare their performance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "Training LSTM...\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "Training LSTM\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "Network: LSTM_Stock_Predictor_V2\n",
            "============================================================\n",
            "Layer Name           Type                 Parameters          \n",
            "------------------------------------------------------------\n",
            "lstm1                LSTM                 13312               \n",
            "batchnorm1           BatchNormalization   64                  \n",
            "dropout1             Dropout              0                   \n",
            "dense1               Dense                528                 \n",
            "batchnorm2           BatchNormalization   32                  \n",
            "relu1                ReLU                 0                   \n",
            "dropout2             Dropout              0                   \n",
            "dense2               Dense                17                  \n",
            "sigmoid1             Sigmoid              0                   \n",
            "------------------------------------------------------------\n",
            "Total parameters: 13953\n",
            "============================================================\n",
            "\n",
            "\n",
            "‚úì Using weighted focal loss (gamma=1.0)\n",
            "  Class 0 weight: 1.3181 (boosted), Class 1 weight: 0.9177\n",
            "  Learning rate: 0.0002\n",
            "\n",
            "Training for up to 30 epochs...\n",
            "Early stopping based on validation accuracy (patience=5)\n",
            "Epoch 1/30 | Train: 50.0% | Val: 42.4% | Gap: +7.5% | Class0: 97.4% | Class1: 2.1%\n",
            "Epoch 3/30 | Train: 51.2% | Val: 43.3% | Gap: +7.9% | Class0: 96.9% | Class1: 4.0%\n",
            "Epoch 6/30 | Train: 57.5% | Val: 44.6% | Gap: +12.8% | Class0: 83.0% | Class1: 16.5%\n",
            "Epoch 9/30 | Train: 62.2% | Val: 49.8% | Gap: +12.4% | Class0: 54.5% | Class1: 46.3%\n",
            "Epoch 12/30 | Train: 65.0% | Val: 51.4% | Gap: +13.7% | Class0: 51.9% | Class1: 51.0%\n",
            "Epoch 15/30 | Train: 67.7% | Val: 52.6% | Gap: +15.1% | Class0: 43.5% | Class1: 59.2%\n",
            "Epoch 18/30 | Train: 68.8% | Val: 54.0% | Gap: +14.8% | Class0: 41.2% | Class1: 63.3%\n",
            "Epoch 21/30 | Train: 70.4% | Val: 53.6% | Gap: +16.7% | Class0: 37.5% | Class1: 65.5%\n",
            "\n",
            "Early stopping at epoch 23 (no improvement for 5 epochs)\n",
            "\n",
            "‚úì Training completed!\n",
            "  Best Val Accuracy: 53.98% (Loss: 0.4410)\n",
            "  Final Val Accuracy: 55.15%\n",
            "  Train Accuracy: 69.88%\n",
            "  Class 0 (Lower) Accuracy: 32.93%\n",
            "  Class 1 (Higher) Accuracy: 71.42%\n",
            "‚úì Memory freed after LSTM training\n",
            "\n",
            "============================================================\n",
            "Training GRU (no focal loss for stability)...\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "Training GRU\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "Network: GRU_Stock_Predictor_V3\n",
            "============================================================\n",
            "Layer Name           Type                 Parameters          \n",
            "------------------------------------------------------------\n",
            "gru1                 GRU                  26112               \n",
            "dropout1             Dropout              0                   \n",
            "dense1               Dense                2080                \n",
            "relu1                ReLU                 0                   \n",
            "dropout2             Dropout              0                   \n",
            "dense2               Dense                33                  \n",
            "sigmoid1             Sigmoid              0                   \n",
            "------------------------------------------------------------\n",
            "Total parameters: 28225\n",
            "============================================================\n",
            "\n",
            "\n",
            "‚úì Using weighted BCE (no focal loss - more stable for some architectures)\n",
            "  Class 0 weight: 1.3181 (boosted), Class 1 weight: 0.9177\n",
            "  Learning rate: 0.0005\n",
            "\n",
            "Training for up to 30 epochs...\n",
            "Early stopping based on validation accuracy (patience=5)\n",
            "Epoch 1/30 | Train: 50.1% | Val: 42.6% | Gap: +7.5% | Class0: 96.6% | Class1: 3.0%\n",
            "Epoch 3/30 | Train: 50.9% | Val: 43.0% | Gap: +7.9% | Class0: 93.5% | Class1: 6.1%\n",
            "Epoch 6/30 | Train: 51.6% | Val: 42.8% | Gap: +8.8% | Class0: 93.4% | Class1: 5.7%\n",
            "Epoch 9/30 | Train: 51.4% | Val: 43.3% | Gap: +8.0% | Class0: 97.3% | Class1: 3.8%\n",
            "Epoch 12/30 | Train: 51.2% | Val: 42.9% | Gap: +8.3% | Class0: 93.2% | Class1: 6.1%\n",
            "\n",
            "Early stopping at epoch 14 (no improvement for 5 epochs)\n",
            "\n",
            "‚úì Training completed!\n",
            "  Best Val Accuracy: 43.33% (Loss: 0.7707)\n",
            "  Final Val Accuracy: 43.33%\n",
            "  Train Accuracy: 51.37%\n",
            "  Class 0 (Lower) Accuracy: 97.32%\n",
            "  Class 1 (Higher) Accuracy: 3.77%\n",
            "‚úì Memory freed after GRU training\n",
            "\n",
            "============================================================\n",
            "Training Conv1D...\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "Training Conv1D\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "Network: Conv1D_Stock_Predictor_V2\n",
            "============================================================\n",
            "Layer Name           Type                 Parameters          \n",
            "------------------------------------------------------------\n",
            "conv1                Conv1D               6848                \n",
            "batchnorm1           BatchNormalization   64                  \n",
            "relu1                ReLU                 0                   \n",
            "pool1                MaxPooling1D         0                   \n",
            "dropout1             Dropout              0                   \n",
            "conv2                Conv1D               6208                \n",
            "batchnorm2           BatchNormalization   128                 \n",
            "relu2                ReLU                 0                   \n",
            "pool2                MaxPooling1D         0                   \n",
            "flatten              Flatten              0                   \n",
            "dense1               Dense                10272               \n",
            "batchnorm3           BatchNormalization   64                  \n",
            "relu3                ReLU                 0                   \n",
            "dropout2             Dropout              0                   \n",
            "dense2               Dense                33                  \n",
            "sigmoid1             Sigmoid              0                   \n",
            "------------------------------------------------------------\n",
            "Total parameters: 23617\n",
            "============================================================\n",
            "\n",
            "\n",
            "‚úì Using weighted focal loss (gamma=1.0)\n",
            "  Class 0 weight: 1.3181 (boosted), Class 1 weight: 0.9177\n",
            "  Learning rate: 0.0002\n",
            "\n",
            "Training for up to 30 epochs...\n",
            "Early stopping based on validation accuracy (patience=5)\n",
            "Epoch 1/30 | Train: 50.1% | Val: 43.5% | Gap: +6.6% | Class0: 89.8% | Class1: 9.6%\n",
            "Epoch 3/30 | Train: 50.3% | Val: 43.1% | Gap: +7.2% | Class0: 92.7% | Class1: 6.7%\n",
            "Epoch 6/30 | Train: 51.0% | Val: 45.5% | Gap: +5.5% | Class0: 81.0% | Class1: 19.4%\n",
            "Epoch 9/30 | Train: 52.4% | Val: 44.2% | Gap: +8.3% | Class0: 86.3% | Class1: 13.3%\n",
            "\n",
            "Early stopping at epoch 11 (no improvement for 5 epochs)\n",
            "\n",
            "‚úì Training completed!\n",
            "  Best Val Accuracy: 45.47% (Loss: 0.3926)\n",
            "  Final Val Accuracy: 43.88%\n",
            "  Train Accuracy: 47.47%\n",
            "  Class 0 (Lower) Accuracy: 92.61%\n",
            "  Class 1 (Higher) Accuracy: 8.18%\n",
            "‚úì Memory freed after Conv1D training\n",
            "\n",
            "============================================================\n",
            "Training Hybrid (Conv1D + LSTM)...\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "Training Hybrid\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "Network: Hybrid_ConvLSTM_V2\n",
            "============================================================\n",
            "Layer Name           Type                 Parameters          \n",
            "------------------------------------------------------------\n",
            "conv1                Conv1D               6848                \n",
            "batchnorm1           BatchNormalization   64                  \n",
            "relu1                ReLU                 0                   \n",
            "pool1                MaxPooling1D         0                   \n",
            "dropout1             Dropout              0                   \n",
            "lstm1                LSTM                 8320                \n",
            "batchnorm2           BatchNormalization   64                  \n",
            "dropout2             Dropout              0                   \n",
            "dense1               Dense                528                 \n",
            "relu2                ReLU                 0                   \n",
            "dropout3             Dropout              0                   \n",
            "dense2               Dense                17                  \n",
            "sigmoid1             Sigmoid              0                   \n",
            "------------------------------------------------------------\n",
            "Total parameters: 15841\n",
            "============================================================\n",
            "\n",
            "\n",
            "‚úì Using weighted focal loss (gamma=1.0)\n",
            "  Class 0 weight: 1.3181 (boosted), Class 1 weight: 0.9177\n",
            "  Learning rate: 0.0002\n",
            "\n",
            "Training for up to 30 epochs...\n",
            "Early stopping based on validation accuracy (patience=5)\n",
            "Epoch 1/30 | Train: 46.9% | Val: 42.6% | Gap: +4.3% | Class0: 97.5% | Class1: 2.4%\n",
            "Epoch 3/30 | Train: 47.4% | Val: 42.4% | Gap: +5.0% | Class0: 99.7% | Class1: 0.4%\n",
            "Epoch 6/30 | Train: 50.7% | Val: 48.4% | Gap: +2.4% | Class0: 78.7% | Class1: 26.1%\n",
            "Epoch 9/30 | Train: 54.3% | Val: 50.3% | Gap: +4.0% | Class0: 57.8% | Class1: 44.8%\n",
            "Epoch 12/30 | Train: 56.3% | Val: 53.5% | Gap: +2.8% | Class0: 50.0% | Class1: 56.2%\n",
            "Epoch 15/30 | Train: 57.2% | Val: 51.0% | Gap: +6.2% | Class0: 60.9% | Class1: 43.7%\n",
            "Epoch 18/30 | Train: 59.7% | Val: 54.6% | Gap: +5.1% | Class0: 47.4% | Class1: 59.8%\n",
            "Epoch 21/30 | Train: 59.5% | Val: 53.7% | Gap: +5.8% | Class0: 51.7% | Class1: 55.1%\n",
            "\n",
            "Early stopping at epoch 23 (no improvement for 5 epochs)\n",
            "\n",
            "‚úì Training completed!\n",
            "  Best Val Accuracy: 54.57% (Loss: 0.4175)\n",
            "  Final Val Accuracy: 54.87%\n",
            "  Train Accuracy: 60.05%\n",
            "  Class 0 (Lower) Accuracy: 44.49%\n",
            "  Class 1 (Higher) Accuracy: 62.48%\n",
            "‚úì Memory freed after Hybrid training\n",
            "\n",
            "============================================================\n",
            "All Models Trained!\n",
            "============================================================\n",
            "üèÜ Best Model: Hybrid (Val Accuracy: 54.57%)\n"
          ]
        }
      ],
      "source": [
        "import gc\n",
        "import sys\n",
        "\n",
        "models_results = {}\n",
        "trained_models = {}\n",
        "best_model_name = None\n",
        "best_model = None\n",
        "best_trainer = None\n",
        "best_val_acc = 0.0\n",
        "\n",
        "# Train LSTM (with focal loss)\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Training LSTM...\")\n",
        "print(\"=\"*60)\n",
        "lstm_model, lstm_trainer, lstm_results = build_and_train_model(\n",
        "    \"LSTM\", build_lstm_model, X_train, y_train, X_val, y_val,\n",
        "    use_focal_loss=True\n",
        ")\n",
        "models_results['LSTM'] = lstm_results\n",
        "if lstm_results['val_accuracy'] > best_val_acc:\n",
        "    best_val_acc = lstm_results['val_accuracy']\n",
        "    best_model_name = 'LSTM'\n",
        "    best_model = lstm_model\n",
        "    best_trainer = lstm_trainer\n",
        "trained_models['LSTM'] = (lstm_model, lstm_trainer)\n",
        "del lstm_model, lstm_trainer\n",
        "gc.collect()\n",
        "print(\"‚úì Memory freed after LSTM training\")\n",
        "\n",
        "# Train GRU (WITHOUT focal loss - prevents collapse, with higher LR)\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Training GRU (no focal loss for stability)...\")\n",
        "print(\"=\"*60)\n",
        "gc.collect()\n",
        "gru_model, gru_trainer, gru_results = build_and_train_model(\n",
        "    \"GRU\", build_gru_model, X_train, y_train, X_val, y_val,\n",
        "    use_focal_loss=False,  # Disable focal loss to prevent collapse\n",
        "    custom_lr=0.0005  # Higher LR for GRU to escape local minima\n",
        ")\n",
        "models_results['GRU'] = gru_results\n",
        "if gru_results['val_accuracy'] > best_val_acc:\n",
        "    best_val_acc = gru_results['val_accuracy']\n",
        "    best_model_name = 'GRU'\n",
        "    best_model = gru_model\n",
        "    best_trainer = gru_trainer\n",
        "trained_models['GRU'] = (gru_model, gru_trainer)\n",
        "del gru_model, gru_trainer\n",
        "gc.collect()\n",
        "print(\"‚úì Memory freed after GRU training\")\n",
        "\n",
        "# Train Conv1D\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Training Conv1D...\")\n",
        "print(\"=\"*60)\n",
        "gc.collect()\n",
        "conv_model, conv_trainer, conv_results = build_and_train_model(\n",
        "    \"Conv1D\", build_conv1d_model, X_train, y_train, X_val, y_val,\n",
        "    use_focal_loss=True\n",
        ")\n",
        "models_results['Conv1D'] = conv_results\n",
        "if conv_results['val_accuracy'] > best_val_acc:\n",
        "    best_val_acc = conv_results['val_accuracy']\n",
        "    best_model_name = 'Conv1D'\n",
        "    best_model = conv_model\n",
        "    best_trainer = conv_trainer\n",
        "trained_models['Conv1D'] = (conv_model, conv_trainer)\n",
        "del conv_model, conv_trainer\n",
        "gc.collect()\n",
        "print(\"‚úì Memory freed after Conv1D training\")\n",
        "\n",
        "# Train Hybrid (Conv1D + LSTM)\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Training Hybrid (Conv1D + LSTM)...\")\n",
        "print(\"=\"*60)\n",
        "gc.collect()\n",
        "hybrid_model, hybrid_trainer, hybrid_results = build_and_train_model(\n",
        "    \"Hybrid\", build_hybrid_model, X_train, y_train, X_val, y_val,\n",
        "    use_focal_loss=True\n",
        ")\n",
        "models_results['Hybrid'] = hybrid_results\n",
        "if hybrid_results['val_accuracy'] > best_val_acc:\n",
        "    best_val_acc = hybrid_results['val_accuracy']\n",
        "    best_model_name = 'Hybrid'\n",
        "    best_model = hybrid_model\n",
        "    best_trainer = hybrid_trainer\n",
        "trained_models['Hybrid'] = (hybrid_model, hybrid_trainer)\n",
        "del hybrid_model, hybrid_trainer\n",
        "gc.collect()\n",
        "print(\"‚úì Memory freed after Hybrid training\")\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"All Models Trained!\")\n",
        "print(f\"{'='*60}\")\n",
        "print(f\"üèÜ Best Model: {best_model_name} (Val Accuracy: {best_val_acc:.2f}%)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Compare Models and Select Best\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "Model Comparison\n",
            "============================================================\n",
            "\n",
            " Model  Val Loss  Val Accuracy  Train Accuracy  Class 0 Acc  Class 1 Acc   Balance\n",
            "Hybrid  0.417458     54.573333       60.052000    44.489989    62.481229 82.008760\n",
            "  LSTM  0.440989     53.980000       69.881333    32.933943    71.421971 61.511972\n",
            "Conv1D  0.392557     45.466667       47.466667    92.606022     8.178353 15.572330\n",
            "   GRU  0.770712     43.326667       51.368000    97.319880     3.765739  6.445859\n",
            "\n",
            "============================================================\n",
            "üèÜ Best Model (by Val Accuracy): Hybrid\n",
            "============================================================\n",
            "  Validation Loss: 0.4175\n",
            "  Validation Accuracy: 54.57%\n",
            "  Train Accuracy: 60.05%\n",
            "  Generalization Gap: 5.48%\n",
            "\n",
            "============================================================\n",
            "Detailed Evaluation: Hybrid\n",
            "============================================================\n",
            "\n",
            "Metrics:\n",
            "  Precision: 0.6057\n",
            "  Recall: 0.6248\n",
            "  F1-Score: 0.6151\n",
            "\n",
            "Confusion Matrix:\n",
            "                Predicted\n",
            "              ‚Üì (0)   ‚Üë (1)\n",
            "Actual ‚Üì (0)    2822    3521\n",
            "       ‚Üë (1)    3248    5409\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     ‚Üì Lower       0.46      0.44      0.45      6343\n",
            "    ‚Üë Higher       0.61      0.62      0.62      8657\n",
            "\n",
            "    accuracy                           0.55     15000\n",
            "   macro avg       0.54      0.53      0.53     15000\n",
            "weighted avg       0.55      0.55      0.55     15000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Model Comparison\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "comparison_df = pd.DataFrame([\n",
        "    {\n",
        "        'Model': name,\n",
        "        'Val Loss': results['val_loss'],\n",
        "        'Val Accuracy': results['val_accuracy'],\n",
        "        'Train Accuracy': results['train_accuracy'],\n",
        "        'Class 0 Acc': results.get('class_0_accuracy', 0),\n",
        "        'Class 1 Acc': results.get('class_1_accuracy', 0)\n",
        "    }\n",
        "    for name, results in models_results.items()\n",
        "]).sort_values('Val Accuracy', ascending=False)  # Sort by accuracy (descending)\n",
        "\n",
        "# Calculate balance score (how well balanced are predictions between classes)\n",
        "comparison_df['Balance'] = 100 - abs(comparison_df['Class 0 Acc'] - comparison_df['Class 1 Acc'])\n",
        "\n",
        "print(\"\\n\" + comparison_df.to_string(index=False))\n",
        "\n",
        "# Select best model by validation accuracy (more meaningful for classification)\n",
        "# Override the loss-based selection with accuracy-based selection\n",
        "best_model_name_by_accuracy = comparison_df.iloc[0]['Model']\n",
        "best_model_by_accuracy, best_trainer_by_accuracy = trained_models[best_model_name_by_accuracy]\n",
        "\n",
        "# Use accuracy-based selection\n",
        "best_model_name = best_model_name_by_accuracy\n",
        "best_model = best_model_by_accuracy\n",
        "best_trainer = best_trainer_by_accuracy\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"üèÜ Best Model (by Val Accuracy): {best_model_name}\")\n",
        "print(f\"{'='*60}\")\n",
        "best_row = comparison_df[comparison_df['Model'] == best_model_name].iloc[0]\n",
        "print(f\"  Validation Loss: {best_row['Val Loss']:.4f}\")\n",
        "print(f\"  Validation Accuracy: {best_row['Val Accuracy']:.2f}%\")\n",
        "print(f\"  Train Accuracy: {best_row['Train Accuracy']:.2f}%\")\n",
        "print(f\"  Generalization Gap: {best_row['Train Accuracy'] - best_row['Val Accuracy']:.2f}%\")\n",
        "\n",
        "# Detailed evaluation of best model\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"Detailed Evaluation: {best_model_name}\")\n",
        "print(f\"{'='*60}\")\n",
        "\n",
        "# Make predictions\n",
        "y_pred = best_trainer.predict(X_val)\n",
        "y_pred_binary = (y_pred > 0.5).astype(int).flatten()\n",
        "\n",
        "# Calculate metrics\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
        "\n",
        "precision = precision_score(y_val, y_pred_binary, zero_division=0)\n",
        "recall = recall_score(y_val, y_pred_binary, zero_division=0)\n",
        "f1 = f1_score(y_val, y_pred_binary, zero_division=0)\n",
        "\n",
        "print(f\"\\nMetrics:\")\n",
        "print(f\"  Precision: {precision:.4f}\")\n",
        "print(f\"  Recall: {recall:.4f}\")\n",
        "print(f\"  F1-Score: {f1:.4f}\")\n",
        "\n",
        "print(f\"\\nConfusion Matrix:\")\n",
        "cm = confusion_matrix(y_val, y_pred_binary)\n",
        "print(f\"                Predicted\")\n",
        "print(f\"              ‚Üì (0)   ‚Üë (1)\")\n",
        "print(f\"Actual ‚Üì (0)  {cm[0,0]:6d}  {cm[0,1]:6d}\")\n",
        "print(f\"       ‚Üë (1)  {cm[1,0]:6d}  {cm[1,1]:6d}\")\n",
        "\n",
        "print(f\"\\nClassification Report:\")\n",
        "print(classification_report(y_val, y_pred_binary, target_names=['‚Üì Lower', '‚Üë Higher']))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Saving models to ../models...\n",
            "‚úì Saved best model (Hybrid) parameters\n",
            "‚úì Model comparison saved\n",
            "\n",
            "‚úì Models saved!\n",
            "‚úì Best model: Hybrid\n"
          ]
        }
      ],
      "source": [
        "model_dir = Path(\"../models\")\n",
        "model_dir.mkdir(exist_ok=True)\n",
        "\n",
        "print(f\"\\nSaving models to {model_dir}...\")\n",
        "# save the best model\n",
        "best_params = best_model.get_params()\n",
        "with open(model_dir / \"best_model_params.pkl\", \"wb\") as f:\n",
        "    pickle.dump(best_params, f)\n",
        "print(f\"‚úì Saved best model ({best_model_name}) parameters\")\n",
        "\n",
        "with open(model_dir / \"model_comparison.pkl\", \"wb\") as f:\n",
        "    pickle.dump({\n",
        "        'comparison': comparison_df.to_dict('records'),\n",
        "        'best_model': best_model_name,\n",
        "        'models_results': models_results,\n",
        "        'sequence_length': SEQUENCE_LENGTH,\n",
        "        'feature_cols': feature_cols\n",
        "    }, f)\n",
        "print(f\"‚úì Model comparison saved\")\n",
        "\n",
        "print(f\"\\n‚úì Models saved!\")\n",
        "print(f\"‚úì Best model: {best_model_name}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
